import matplotlib
import numpy as np
import pandas as pd
from sklearn.linear_model import LinearRegression
from sklearn.linear_model import Lasso,LassoCV
from sklearn.linear_model import Ridge
from sklearn.linear_model import ElasticNet
from sklearn.ensemble import VotingRegressor
from sklearn.preprocessing import StandardScaler
import statsmodels.api as sm
from sklearn.metrics import r2_score
from scipy.stats import pearsonr
from sklearn.model_selection import train_test_split, GridSearchCV, RepeatedKFold, cross_val_score
from sklearn.model_selection import cross_validate    ## å¯¼å…¥äº¤å‰éªŒè¯åŒ…
from sklearn.model_selection import  cross_val_score   ## å¯¼å…¥äº¤å‰éªŒè¯åŒ…
from sklearn.model_selection import KFold   ## å¯¼å…¥KFoldåˆ†ç¦»å™¨
from sklearn.metrics import roc_auc_score,make_scorer
from scipy.stats import pearsonr
from sklearn import linear_model
import matplotlib.pyplot as plt
from numpy import genfromtxt
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.metrics import mean_squared_error
from sklearn.svm import SVR      ## å¯¼å…¥æ”¯æŒå‘é‡æœºåŒ…
from sklearn.model_selection import KFold
# import xgboost as xgb
# from xgboost import XGBRegressor
# import lightgbm as lgb
from mlxtend.feature_selection import SequentialFeatureSelector as SFS
from sklearn.model_selection import cross_val_score, KFold
from scipy import stats
from statsmodels.tools.tools import add_constant
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
from joblib import load
import seaborn as sns
from scipy.stats import t
from matplotlib import font_manager
from matplotlib.colors import TwoSlopeNorm, ListedColormap, LinearSegmentedColormap
from statsmodels.stats.outliers_influence import variance_inflation_factor
from joblib import dump

from statsmodels.stats.outliers_influence import variance_inflation_factor
from statsmodels.tools.tools import add_constant
from statsmodels.stats.stattools import durbin_watson

import warnings
warnings.filterwarnings("ignore")


## å¯¼å…¥æ•°æ®
X_train = pd.read_csv("C:/Users/zjl__/Desktop/output_zhaojialu/output/SummaryStatistic/X_train_All3D_NoScaled.csv",sep=",")
X_train.set_index(X_train.columns[0], inplace= True)  ## å°†æ•°æ®æ¡†çš„ç¬¬ä¸€åˆ—ä½œä¸ºæ•°æ®æ¡†çš„è¡Œå
name = X_train.columns.tolist()    ## æå–æ•°æ®æ¡†çš„åˆ—å
name = [item.replace('äººä½“å¤–è§‚æµ‹é‡.ä¸‰ç»´äººä½“æ‰«æåˆ†æç³»ç»Ÿ.', '') for item in name]    ## åˆ é™¤åˆ—åä¸­â€œäººä½“å¤–è§‚æµ‹é‡-ä¸‰ç»´äººä½“æ‰«æåˆ†æç³»ç»Ÿ:â€çš„éƒ¨åˆ†
name = [item.replace('.cm.', '') for item in name]    ## åˆ é™¤åˆ—åä¸­â€œ.cm.â€çš„éƒ¨åˆ†
X_train.columns = name   ## æ›¿æ¢æ•°æ®æ¡†çš„åˆ—åä¸ºæœ€ç®€å•åå­—
# print(X_train)
X_train = X_train.drop(columns = ['height', 'Waist_To_Hip_Ratio.x.100.'])
print(X_train)

X_test = pd.read_csv("C:/Users/zjl__/Desktop/output_zhaojialu/output/SummaryStatistic/X_test_All3D_NoScaled.csv",sep=",")
X_test.set_index(X_test.columns[0], inplace= True)  ## å°†æ•°æ®æ¡†çš„ç¬¬ä¸€åˆ—ä½œä¸ºæ•°æ®æ¡†çš„è¡Œå
X_test.columns = name   ## æ›¿æ¢æ•°æ®æ¡†çš„åˆ—åä¸ºæœ€ç®€å•åå­—
X_test = X_test.drop(columns = ['height', 'Waist_To_Hip_Ratio.x.100.'])
# print(X_test)

y_train = pd.read_csv("C:/Users/zjl__/Desktop/output_zhaojialu/output/SummaryStatistic/y_train_All3D_NoScaled.csv",sep=",")
y_train.set_index(y_train.columns[0], inplace= True)  ## å°†æ•°æ®æ¡†çš„ç¬¬ä¸€åˆ—ä½œä¸ºæ•°æ®æ¡†çš„è¡Œå

y_test = pd.read_csv("C:/Users/zjl__/Desktop/output_zhaojialu/output/SummaryStatistic/y_test_All3D_NoScaled.csv",sep=",")
y_test.set_index(y_test.columns[0], inplace= True)  ## å°†æ•°æ®æ¡†çš„ç¬¬ä¸€åˆ—ä½œä¸ºæ•°æ®æ¡†çš„è¡Œå

## åŠ è½½ä¹‹å‰ç”¨è®­ç»ƒé›†è®­ç»ƒçš„å¤šä¸ªæ¨¡å‹åŠå‚æ•°
AA_20240105_All_models = load('C:/Users/zjl__/Desktop/output_zhaojialu/output/ForwardRegression1/AA_20240105_All_models.joblib')

"""
def feature_selection_process(data, target_col, significance_level=0.05, vif_threshold=10):
    """
    ç‰¹å¾é€‰æ‹©æµç¨‹ï¼šå…ˆè¿‡æ»¤ç‰¹å®šå…³é”®è¯ï¼Œç„¶åå¾ªç¯è¿›è¡Œæ˜¾è‘—æ€§æ£€éªŒå’ŒVIFæ£€éªŒ
    
    å‚æ•°:
    data: åŒ…å«ç‰¹å¾å’Œç›®æ ‡å˜é‡çš„DataFrame
    target_col: ç›®æ ‡å˜é‡åˆ—å
    significance_level: æ˜¾è‘—æ€§æ°´å¹³ï¼Œé»˜è®¤0.05
    vif_threshold: VIFé˜ˆå€¼ï¼Œé»˜è®¤10
    
    è¿”å›:
    æœ€ç»ˆé€‰æ‹©çš„ç‰¹å¾åˆ—è¡¨å’Œæœ€ç»ˆçš„å›å½’æ¨¡å‹
    """
    
    # 1. ç­›é€‰æ‰åŒ…å«ç‰¹å®šå…³é”®è¯çš„ç‰¹å¾
    exclude_keywords = ['3D', 'è…°å¸¦', 'å‚ç›´', 'å†…éƒ¨']
    initial_features = [col for col in data.columns 
                       if col != target_col and 
                       not any(keyword in col for keyword in exclude_keywords)]
    
    print(f"åˆå§‹ç‰¹å¾æ•°é‡: {len(initial_features)}")
    print(f"æ’é™¤çš„ç‰¹å¾: {[col for col in data.columns if col != target_col and col not in initial_features]}")
    
    current_features = initial_features.copy()
    iteration = 1
    
    while True:
        print(f"\n=== ç¬¬ {iteration} æ¬¡è¿­ä»£ ===")
        
        # å‡†å¤‡æ•°æ®
        X = data[current_features]
        y = data[target_col]
        X_with_const = add_constant(X)
        
        # æ‹Ÿåˆå›å½’æ¨¡å‹
        try:
            model = sm.OLS(y, X_with_const).fit()
        except Exception as e:
            print(f"å›å½’æ¨¡å‹æ‹Ÿåˆå¤±è´¥: {e}")
            break
            
        # 2. æ£€æŸ¥è‡ªå˜é‡æ˜¾è‘—æ€§
        pvalues = model.pvalues.drop('const')
        insignificant_features = pvalues[pvalues > significance_level].index.tolist()
        
        # 3. è®¡ç®—VIFå€¼
        vif_data = pd.DataFrame()
        vif_data["feature"] = X_with_const.columns
        vif_data["VIF"] = [variance_inflation_factor(X_with_const.values, i) 
                          for i in range(X_with_const.shape[1])]
        

        vif_data = vif_data[vif_data['feature'] != 'const']
        
        high_vif_features = vif_data[vif_data["VIF"] > vif_threshold]
        
        print(f"å½“å‰ç‰¹å¾æ•°é‡: {len(current_features)}")
        print(f"ä¸æ˜¾è‘—çš„ç‰¹å¾ (p > {significance_level}): {insignificant_features}")
        print(f"VIF > {vif_threshold} çš„ç‰¹å¾:")
        print(high_vif_features)
        
        # æ£€æŸ¥ç»ˆæ­¢æ¡ä»¶
        if len(insignificant_features) == 0 and len(high_vif_features) == 0:
            print("\nâœ… è¾¾åˆ°ç»ˆæ­¢æ¡ä»¶: æ‰€æœ‰ç‰¹å¾éƒ½æ˜¾è‘—ä¸”VIF < 10")
            break
            
        # å†³å®šè¦ç§»é™¤çš„ç‰¹å¾
        features_to_remove = []
        
        # ä¼˜å…ˆç§»é™¤ä¸æ˜¾è‘—çš„ç‰¹å¾
        if len(insignificant_features) > 0:
            # ç§»é™¤på€¼æœ€å¤§çš„ä¸æ˜¾è‘—ç‰¹å¾
            max_p_feature = pvalues.loc[insignificant_features].idxmax()
            features_to_remove.append(max_p_feature)
            print(f"ç§»é™¤ä¸æ˜¾è‘—ç‰¹å¾: {max_p_feature} (p-value: {pvalues[max_p_feature]:.4f})")
        
        # å¦‚æœæ²¡æœ‰ä¸æ˜¾è‘—ç‰¹å¾ä½†æœ‰é«˜VIFç‰¹å¾ï¼Œç§»é™¤VIFæœ€å¤§çš„ç‰¹å¾
        elif len(high_vif_features) > 0:
            max_vif_feature = high_vif_features.loc[high_vif_features["VIF"].idxmax(), "feature"]
            features_to_remove.append(max_vif_feature)
            print(f"ç§»é™¤é«˜VIFç‰¹å¾: {max_vif_feature} (VIF: {high_vif_features.loc[high_vif_features['VIF'].idxmax(), 'VIF']:.2f})")
        
        # ç§»é™¤ç‰¹å¾
        for feature in features_to_remove:
            current_features.remove(feature)
            
        # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰ç‰¹å¾å‰©ä½™
        if len(current_features) == 0:
            print("âŒ æ‰€æœ‰ç‰¹å¾éƒ½è¢«ç§»é™¤äº†ï¼")
            break
            
        iteration += 1
        
        # é˜²æ­¢æ— é™å¾ªç¯
        if iteration > 50:
            print("âš ï¸ è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°ï¼Œå¼ºåˆ¶ç»ˆæ­¢")
            break
    
    # è¾“å‡ºæœ€ç»ˆç»“æœ
    print(f"\nğŸ¯ æœ€ç»ˆé€‰æ‹©çš„ç‰¹å¾ ({len(current_features)}ä¸ª):")
    for feature in current_features:
        p_value = model.pvalues[feature] if feature in model.pvalues else 'N/A'
        vif_value = vif_data[vif_data['feature'] == feature]['VIF'].values[0] if feature in vif_data['feature'].values else 'N/A'
        print(f"  {feature}: p-value={p_value:.4f}, VIF={vif_value}")
    
    return current_features, model

def feature_selection_process(data, target_col, significance_level=0.05, vif_threshold=10, protected_feature='æ ‡å‡†ç«™å§¿è…°å›´'):
    """
    ç‰¹å¾é€‰æ‹©æµç¨‹ï¼šå…ˆè¿‡æ»¤ç‰¹å®šå…³é”®è¯ï¼Œç„¶åå¾ªç¯è¿›è¡Œæ˜¾è‘—æ€§æ£€éªŒå’ŒVIFæ£€éªŒ
    åœ¨æ’é™¤é«˜VIFç‰¹å¾æ—¶ä¼šä¿æŠ¤æŒ‡å®šç‰¹å¾
    
    å‚æ•°:
    data: åŒ…å«ç‰¹å¾å’Œç›®æ ‡å˜é‡çš„DataFrame
    target_col: ç›®æ ‡å˜é‡åˆ—å
    significance_level: æ˜¾è‘—æ€§æ°´å¹³ï¼Œé»˜è®¤0.05
    vif_threshold: VIFé˜ˆå€¼ï¼Œé»˜è®¤10
    protected_feature: è¦ä¿æŠ¤çš„ç‰¹å¾åç§°ï¼Œä¸ä¼šè¢«VIFç­›é€‰ç§»é™¤
    
    è¿”å›:
    æœ€ç»ˆé€‰æ‹©çš„ç‰¹å¾åˆ—è¡¨å’Œæœ€ç»ˆçš„å›å½’æ¨¡å‹
    """
    
    # 1. ç­›é€‰æ‰åŒ…å«ç‰¹å®šå…³é”®è¯çš„ç‰¹å¾
    exclude_keywords = ['3D', 'è…°å¸¦', 'å‚ç›´', 'å†…éƒ¨', 'èº¯å¹²æ‰­', 'è§’åº¦']
    initial_features = [col for col in data.columns 
                       if col != target_col and 
                       not any(keyword in col for keyword in exclude_keywords)]
    
    print(f"åˆå§‹ç‰¹å¾æ•°é‡: {len(initial_features)}")
    print(f"æ’é™¤çš„ç‰¹å¾: {[col for col in data.columns if col != target_col and col not in initial_features]}")
    print(f"ä¿æŠ¤çš„ç‰¹å¾: {protected_feature}")
    
    current_features = initial_features.copy()
    iteration = 1
    
    while True:
        print(f"\n=== ç¬¬ {iteration} æ¬¡è¿­ä»£ ===")
        
        # å‡†å¤‡æ•°æ®
        X = data[current_features]
        y = data[target_col]
        X_with_const = add_constant(X)
        
        # æ‹Ÿåˆå›å½’æ¨¡å‹
        try:
            model = sm.OLS(y, X_with_const).fit()
        except Exception as e:
            print(f"å›å½’æ¨¡å‹æ‹Ÿåˆå¤±è´¥: {e}")
            break
            
        # 2. æ£€æŸ¥è‡ªå˜é‡æ˜¾è‘—æ€§
        pvalues = model.pvalues.drop('const')
        insignificant_features = pvalues[pvalues > significance_level].index.tolist()
        
        # 3. è®¡ç®—VIFå€¼
        vif_data = pd.DataFrame()
        vif_data["feature"] = X_with_const.columns
        vif_data["VIF"] = [variance_inflation_factor(X_with_const.values, i) 
                          for i in range(X_with_const.shape[1])]
        
        vif_data = vif_data[vif_data['feature'] != 'const']
        high_vif_features = vif_data[vif_data["VIF"] > vif_threshold]
        
        print(f"å½“å‰ç‰¹å¾æ•°é‡: {len(current_features)}")
        print(f"ä¸æ˜¾è‘—çš„ç‰¹å¾ (p > {significance_level}): {insignificant_features}")
        print(f"VIF > {vif_threshold} çš„ç‰¹å¾:")
        print(high_vif_features)
        
        # æ£€æŸ¥ç»ˆæ­¢æ¡ä»¶
        if len(insignificant_features) == 0 and len(high_vif_features) == 0:
            print("\nâœ… è¾¾åˆ°ç»ˆæ­¢æ¡ä»¶: æ‰€æœ‰ç‰¹å¾éƒ½æ˜¾è‘—ä¸”VIF < 10")
            break
            
        # å†³å®šè¦ç§»é™¤çš„ç‰¹å¾
        features_to_remove = []
        
        # ä¼˜å…ˆç§»é™¤ä¸æ˜¾è‘—çš„ç‰¹å¾
        if len(insignificant_features) > 0:
            # ç§»é™¤på€¼æœ€å¤§çš„ä¸æ˜¾è‘—ç‰¹å¾
            max_p_feature = pvalues.loc[insignificant_features].idxmax()
            features_to_remove.append(max_p_feature)
            print(f"ç§»é™¤ä¸æ˜¾è‘—ç‰¹å¾: {max_p_feature} (p-value: {pvalues[max_p_feature]:.4f})")
        
        # å¦‚æœæ²¡æœ‰ä¸æ˜¾è‘—ç‰¹å¾ä½†æœ‰é«˜VIFç‰¹å¾ï¼Œç§»é™¤VIFæœ€å¤§çš„ç‰¹å¾ï¼ˆè·³è¿‡ä¿æŠ¤çš„ç‰¹å¾ï¼‰
        elif len(high_vif_features) > 0:
            # åˆ›å»ºå¯ç§»é™¤çš„é«˜VIFç‰¹å¾åˆ—è¡¨ï¼ˆæ’é™¤ä¿æŠ¤çš„ç‰¹å¾ï¼‰
            removable_high_vif_features = high_vif_features[high_vif_features['feature'] != protected_feature]
            
            if len(removable_high_vif_features) > 0:
                # ä»å¯ç§»é™¤çš„ç‰¹å¾ä¸­æ‰¾åˆ°VIFæœ€å¤§çš„ç‰¹å¾
                max_vif_feature = removable_high_vif_features.loc[removable_high_vif_features["VIF"].idxmax(), "feature"]
                features_to_remove.append(max_vif_feature)
                print(f"ç§»é™¤é«˜VIFç‰¹å¾: {max_vif_feature} (VIF: {removable_high_vif_features.loc[removable_high_vif_features['VIF'].idxmax(), 'VIF']:.2f})")
            else:
                # å¦‚æœåªæœ‰ä¿æŠ¤çš„ç‰¹å¾æ˜¯é«˜VIFï¼Œæ£€æŸ¥æ˜¯å¦æœ‰ä¸æ˜¾è‘—ç‰¹å¾å¯ä»¥ç§»é™¤
                if len(insignificant_features) > 0:
                    # ç§»é™¤på€¼æœ€å¤§çš„ä¸æ˜¾è‘—ç‰¹å¾
                    max_p_feature = pvalues.loc[insignificant_features].idxmax()
                    features_to_remove.append(max_p_feature)
                    print(f"åªæœ‰{protected_feature}æ˜¯é«˜VIFç‰¹å¾ï¼Œç§»é™¤ä¸æ˜¾è‘—ç‰¹å¾: {max_p_feature} (p-value: {pvalues[max_p_feature]:.4f})")
                else:
                    print(f"âš ï¸ åªæœ‰{protected_feature}æ˜¯é«˜VIFç‰¹å¾ä¸”æ‰€æœ‰ç‰¹å¾éƒ½æ˜¾è‘—ï¼Œæ— æ³•ç»§ç»­ä¼˜åŒ–VIF")
                    break
        
        # ç§»é™¤ç‰¹å¾
        for feature in features_to_remove:
            if feature in current_features:
                current_features.remove(feature)
                print(f"å·²ç§»é™¤ç‰¹å¾: {feature}")
            else:
                print(f"è­¦å‘Š: ç‰¹å¾ {feature} ä¸åœ¨å½“å‰ç‰¹å¾åˆ—è¡¨ä¸­")
            
        # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰ç‰¹å¾å‰©ä½™
        if len(current_features) == 0:
            print("âŒ æ‰€æœ‰ç‰¹å¾éƒ½è¢«ç§»é™¤äº†ï¼")
            break
            
        iteration += 1
        
        # é˜²æ­¢æ— é™å¾ªç¯
        if iteration > 50:
            print("âš ï¸ è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°ï¼Œå¼ºåˆ¶ç»ˆæ­¢")
            break
    
    # æ£€æŸ¥ä¿æŠ¤çš„ç‰¹å¾æ˜¯å¦è¿˜åœ¨æœ€ç»ˆç‰¹å¾é›†ä¸­
    if protected_feature in current_features:
        print(f"\nâœ… ä¿æŠ¤çš„ç‰¹å¾ '{protected_feature}' å·²ä¿ç•™åœ¨æœ€ç»ˆç‰¹å¾é›†ä¸­")
    else:
        print(f"\nâš ï¸ ä¿æŠ¤çš„ç‰¹å¾ '{protected_feature}' æœªåœ¨æœ€ç»ˆç‰¹å¾é›†ä¸­")
    
    # è¾“å‡ºæœ€ç»ˆç»“æœ
    print(f"\nğŸ¯ æœ€ç»ˆé€‰æ‹©çš„ç‰¹å¾ ({len(current_features)}ä¸ª):")
    # é‡æ–°æ‹Ÿåˆæœ€ç»ˆæ¨¡å‹ä»¥ç¡®ä¿å‡†ç¡®æ€§
    X_final = data[current_features]
    y_final = data[target_col]
    X_final_with_const = add_constant(X_final)
    final_model = sm.OLS(y_final, X_final_with_const).fit()
    
    # è®¡ç®—æœ€ç»ˆVIFå€¼
    final_vif_data = pd.DataFrame()
    final_vif_data["feature"] = X_final_with_const.columns
    final_vif_data["VIF"] = [variance_inflation_factor(X_final_with_const.values, i) 
                           for i in range(X_final_with_const.shape[1])]
    final_vif_data = final_vif_data[final_vif_data['feature'] != 'const']
    
    for feature in current_features:
        p_value = final_model.pvalues[feature]
        vif_value = final_vif_data[final_vif_data['feature'] == feature]['VIF'].values[0]
        protected_indicator = " (ä¿æŠ¤)" if feature == protected_feature else ""
        print(f"  {feature}: p-value={p_value:.4f}, VIF={vif_value:.2f}{protected_indicator}")
    
    return current_features, final_model

## å…ˆVIFï¼Œå†æ˜¾è‘—æ€§

def feature_selection_process2(data, target_col, significance_level=0.05, vif_threshold=10):
    """
    ç‰¹å¾é€‰æ‹©æµç¨‹ï¼šå…ˆè¿‡æ»¤ç‰¹å®šå…³é”®è¯ï¼Œç„¶åå¾ªç¯è¿›è¡ŒVIFæ£€éªŒå’Œæ˜¾è‘—æ€§æ£€éªŒ
    
    å‚æ•°:
    data: åŒ…å«ç‰¹å¾å’Œç›®æ ‡å˜é‡çš„DataFrame
    target_col: ç›®æ ‡å˜é‡åˆ—å
    significance_level: æ˜¾è‘—æ€§æ°´å¹³ï¼Œé»˜è®¤0.05
    vif_threshold: VIFé˜ˆå€¼ï¼Œé»˜è®¤10
    
    è¿”å›:
    æœ€ç»ˆé€‰æ‹©çš„ç‰¹å¾åˆ—è¡¨å’Œæœ€ç»ˆçš„å›å½’æ¨¡å‹
    """
    
    # 1. ç­›é€‰æ‰åŒ…å«ç‰¹å®šå…³é”®è¯çš„ç‰¹å¾
    exclude_keywords = ['3D', 'è…°å¸¦', 'å‚ç›´', 'å†…éƒ¨']
    initial_features = [col for col in data.columns 
                       if col != target_col and 
                       not any(keyword in col for keyword in exclude_keywords)]
    
    print(f"åˆå§‹ç‰¹å¾æ•°é‡: {len(initial_features)}")
    print(f"æ’é™¤çš„ç‰¹å¾: {[col for col in data.columns if col != target_col and col not in initial_features]}")
    
    current_features = initial_features.copy()
    iteration = 1
    
    while True:
        print(f"\n=== ç¬¬ {iteration} æ¬¡è¿­ä»£ ===")
        
        # å‡†å¤‡æ•°æ®
        X = data[current_features]
        y = data[target_col]
        X_with_const = add_constant(X)
        
        # æ‹Ÿåˆå›å½’æ¨¡å‹
        try:
            model = sm.OLS(y, X_with_const).fit()
        except Exception as e:
            print(f"å›å½’æ¨¡å‹æ‹Ÿåˆå¤±è´¥: {e}")
            break
            
        # 2. è®¡ç®—VIFå€¼ï¼ˆå…ˆè¿›è¡ŒVIFç­›é€‰ï¼‰
        vif_data = pd.DataFrame()
        vif_data["feature"] = X_with_const.columns
        vif_data["VIF"] = [variance_inflation_factor(X_with_const.values, i) 
                          for i in range(X_with_const.shape[1])]
        
        vif_data = vif_data[vif_data['feature'] != 'const']
        high_vif_features = vif_data[vif_data["VIF"] > vif_threshold]
        
        # 3. æ£€æŸ¥è‡ªå˜é‡æ˜¾è‘—æ€§ï¼ˆåè¿›è¡Œæ˜¾è‘—æ€§ç­›é€‰ï¼‰
        pvalues = model.pvalues.drop('const')
        insignificant_features = pvalues[pvalues > significance_level].index.tolist()
        
        print(f"å½“å‰ç‰¹å¾æ•°é‡: {len(current_features)}")
        print(f"VIF > {vif_threshold} çš„ç‰¹å¾:")
        print(high_vif_features)
        print(f"ä¸æ˜¾è‘—çš„ç‰¹å¾ (p > {significance_level}): {insignificant_features}")
        
        # æ£€æŸ¥ç»ˆæ­¢æ¡ä»¶
        if len(high_vif_features) == 0 and len(insignificant_features) == 0:
            print("\nâœ… è¾¾åˆ°ç»ˆæ­¢æ¡ä»¶: æ‰€æœ‰ç‰¹å¾VIF < 10ä¸”éƒ½æ˜¾è‘—")
            break
            
        # å†³å®šè¦ç§»é™¤çš„ç‰¹å¾ï¼ˆä¼˜å…ˆç§»é™¤é«˜VIFç‰¹å¾ï¼‰
        features_to_remove = []
        
        # ä¼˜å…ˆç§»é™¤é«˜VIFçš„ç‰¹å¾
        if len(high_vif_features) > 0:
            # ç§»é™¤VIFå€¼æœ€å¤§çš„ç‰¹å¾
            max_vif_feature = high_vif_features.loc[high_vif_features["VIF"].idxmax(), "feature"]
            features_to_remove.append(max_vif_feature)
            print(f"ç§»é™¤é«˜VIFç‰¹å¾: {max_vif_feature} (VIF: {high_vif_features.loc[high_vif_features['VIF'].idxmax(), 'VIF']:.2f})")
        
        # å¦‚æœæ²¡æœ‰é«˜VIFç‰¹å¾ä½†æœ‰ä¸æ˜¾è‘—ç‰¹å¾ï¼Œç§»é™¤på€¼æœ€å¤§çš„ç‰¹å¾
        elif len(insignificant_features) > 0:
            # ç§»é™¤på€¼æœ€å¤§çš„ä¸æ˜¾è‘—ç‰¹å¾
            max_p_feature = pvalues.loc[insignificant_features].idxmax()
            features_to_remove.append(max_p_feature)
            print(f"ç§»é™¤ä¸æ˜¾è‘—ç‰¹å¾: {max_p_feature} (p-value: {pvalues[max_p_feature]:.4f})")
        
        # ç§»é™¤ç‰¹å¾
        for feature in features_to_remove:
            current_features.remove(feature)
            
        # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰ç‰¹å¾å‰©ä½™
        if len(current_features) == 0:
            print("âŒ æ‰€æœ‰ç‰¹å¾éƒ½è¢«ç§»é™¤äº†ï¼")
            break
            
        iteration += 1
        
        # é˜²æ­¢æ— é™å¾ªç¯
        if iteration > 50:
            print("âš ï¸ è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°ï¼Œå¼ºåˆ¶ç»ˆæ­¢")
            break
    
    # è¾“å‡ºæœ€ç»ˆç»“æœ
    print(f"\nğŸ¯ æœ€ç»ˆé€‰æ‹©çš„ç‰¹å¾ ({len(current_features)}ä¸ª):")
    # æ‹Ÿåˆæœ€ç»ˆæ¨¡å‹
    X_final = data[current_features]
    y_final = data[target_col]
    X_final_with_const = add_constant(X_final)
    final_model = sm.OLS(y_final, X_final_with_const).fit()
    
    # è®¡ç®—æœ€ç»ˆVIFå€¼
    final_vif_data = pd.DataFrame()
    final_vif_data["feature"] = X_final_with_const.columns
    final_vif_data["VIF"] = [variance_inflation_factor(X_final_with_const.values, i) 
                           for i in range(X_final_with_const.shape[1])]
    final_vif_data = final_vif_data[final_vif_data['feature'] != 'const']
    
    for feature in current_features:
        p_value = final_model.pvalues[feature]
        vif_value = final_vif_data[final_vif_data['feature'] == feature]['VIF'].values[0]
        print(f"  {feature}: p-value={p_value:.4f}, VIF={vif_value:.2f}")
    
    return current_features, final_model

def feature_selection_process2(data, target_col, significance_level=0.05, vif_threshold=10, protected_feature='æ ‡å‡†ç«™å§¿è…°å›´'):
    """
    ç‰¹å¾é€‰æ‹©æµç¨‹ï¼šå…ˆè¿‡æ»¤ç‰¹å®šå…³é”®è¯ï¼Œç„¶åå¾ªç¯è¿›è¡ŒVIFæ£€éªŒå’Œæ˜¾è‘—æ€§æ£€éªŒ
    åœ¨æ’é™¤é«˜VIFç‰¹å¾æ—¶ä¼šä¿æŠ¤æŒ‡å®šç‰¹å¾
    
    å‚æ•°:
    data: åŒ…å«ç‰¹å¾å’Œç›®æ ‡å˜é‡çš„DataFrame
    target_col: ç›®æ ‡å˜é‡åˆ—å
    significance_level: æ˜¾è‘—æ€§æ°´å¹³ï¼Œé»˜è®¤0.05
    vif_threshold: VIFé˜ˆå€¼ï¼Œé»˜è®¤10
    protected_feature: è¦ä¿æŠ¤çš„ç‰¹å¾åç§°ï¼Œä¸ä¼šè¢«VIFç­›é€‰ç§»é™¤
    
    è¿”å›:
    æœ€ç»ˆé€‰æ‹©çš„ç‰¹å¾åˆ—è¡¨å’Œæœ€ç»ˆçš„å›å½’æ¨¡å‹
    """
    
    # 1. ç­›é€‰æ‰åŒ…å«ç‰¹å®šå…³é”®è¯çš„ç‰¹å¾
    exclude_keywords = ['3D', 'è…°å¸¦', 'å‚ç›´', 'å†…éƒ¨']
    initial_features = [col for col in data.columns 
                       if col != target_col and 
                       not any(keyword in col for keyword in exclude_keywords)]
    
    print(f"åˆå§‹ç‰¹å¾æ•°é‡: {len(initial_features)}")
    print(f"æ’é™¤çš„ç‰¹å¾: {[col for col in data.columns if col != target_col and col not in initial_features]}")
    print(f"ä¿æŠ¤çš„ç‰¹å¾: {protected_feature}")
    
    current_features = initial_features.copy()
    iteration = 1
    
    while True:
        print(f"\n=== ç¬¬ {iteration} æ¬¡è¿­ä»£ ===")
        
        # å‡†å¤‡æ•°æ®
        X = data[current_features]
        y = data[target_col]
        X_with_const = add_constant(X)
        
        # æ‹Ÿåˆå›å½’æ¨¡å‹
        try:
            model = sm.OLS(y, X_with_const).fit()
        except Exception as e:
            print(f"å›å½’æ¨¡å‹æ‹Ÿåˆå¤±è´¥: {e}")
            break
            
        # 2. è®¡ç®—VIFå€¼ï¼ˆå…ˆè¿›è¡ŒVIFç­›é€‰ï¼‰
        vif_data = pd.DataFrame()
        vif_data["feature"] = X_with_const.columns
        vif_data["VIF"] = [variance_inflation_factor(X_with_const.values, i) 
                          for i in range(X_with_const.shape[1])]
        
        vif_data = vif_data[vif_data['feature'] != 'const']
        high_vif_features = vif_data[vif_data["VIF"] > vif_threshold]
        
        # 3. æ£€æŸ¥è‡ªå˜é‡æ˜¾è‘—æ€§ï¼ˆåè¿›è¡Œæ˜¾è‘—æ€§ç­›é€‰ï¼‰
        pvalues = model.pvalues.drop('const')
        insignificant_features = pvalues[pvalues > significance_level].index.tolist()
        
        print(f"å½“å‰ç‰¹å¾æ•°é‡: {len(current_features)}")
        print(f"VIF > {vif_threshold} çš„ç‰¹å¾:")
        print(high_vif_features)
        print(f"ä¸æ˜¾è‘—çš„ç‰¹å¾ (p > {significance_level}): {insignificant_features}")
        
        # æ£€æŸ¥ç»ˆæ­¢æ¡ä»¶
        if len(high_vif_features) == 0 and len(insignificant_features) == 0:
            print("\nâœ… è¾¾åˆ°ç»ˆæ­¢æ¡ä»¶: æ‰€æœ‰ç‰¹å¾VIF < 10ä¸”éƒ½æ˜¾è‘—")
            break
            
        # å†³å®šè¦ç§»é™¤çš„ç‰¹å¾ï¼ˆä¼˜å…ˆç§»é™¤é«˜VIFç‰¹å¾ï¼‰
        features_to_remove = []
        
        # ä¼˜å…ˆç§»é™¤é«˜VIFçš„ç‰¹å¾
        if len(high_vif_features) > 0:
            # åˆ›å»ºå¯ç§»é™¤çš„é«˜VIFç‰¹å¾åˆ—è¡¨ï¼ˆæ’é™¤ä¿æŠ¤çš„ç‰¹å¾ï¼‰
            removable_high_vif_features = high_vif_features[high_vif_features['feature'] != protected_feature]
            
            if len(removable_high_vif_features) > 0:
                # ä»å¯ç§»é™¤çš„ç‰¹å¾ä¸­æ‰¾åˆ°VIFæœ€å¤§çš„ç‰¹å¾
                max_vif_feature = removable_high_vif_features.loc[removable_high_vif_features["VIF"].idxmax(), "feature"]
                features_to_remove.append(max_vif_feature)
                print(f"ç§»é™¤é«˜VIFç‰¹å¾: {max_vif_feature} (VIF: {removable_high_vif_features.loc[removable_high_vif_features['VIF'].idxmax(), 'VIF']:.2f})")
            else:
                # å¦‚æœåªæœ‰ä¿æŠ¤çš„ç‰¹å¾æ˜¯é«˜VIFï¼Œæ£€æŸ¥æ˜¯å¦æœ‰ä¸æ˜¾è‘—ç‰¹å¾å¯ä»¥ç§»é™¤
                if len(insignificant_features) > 0:
                    # ç§»é™¤på€¼æœ€å¤§çš„ä¸æ˜¾è‘—ç‰¹å¾
                    max_p_feature = pvalues.loc[insignificant_features].idxmax()
                    features_to_remove.append(max_p_feature)
                    print(f"åªæœ‰{protected_feature}æ˜¯é«˜VIFç‰¹å¾ï¼Œç§»é™¤ä¸æ˜¾è‘—ç‰¹å¾: {max_p_feature} (p-value: {pvalues[max_p_feature]:.4f})")
                else:
                    print(f"âš ï¸ åªæœ‰{protected_feature}æ˜¯é«˜VIFç‰¹å¾ä¸”æ‰€æœ‰ç‰¹å¾éƒ½æ˜¾è‘—ï¼Œæ— æ³•ç»§ç»­ä¼˜åŒ–VIF")
                    break
        
        # å¦‚æœæ²¡æœ‰é«˜VIFç‰¹å¾ä½†æœ‰ä¸æ˜¾è‘—ç‰¹å¾ï¼Œç§»é™¤på€¼æœ€å¤§çš„ç‰¹å¾
        elif len(insignificant_features) > 0:
            # ç§»é™¤på€¼æœ€å¤§çš„ä¸æ˜¾è‘—ç‰¹å¾
            max_p_feature = pvalues.loc[insignificant_features].idxmax()
            features_to_remove.append(max_p_feature)
            print(f"ç§»é™¤ä¸æ˜¾è‘—ç‰¹å¾: {max_p_feature} (p-value: {pvalues[max_p_feature]:.4f})")
        
        # ç§»é™¤ç‰¹å¾
        for feature in features_to_remove:
            if feature in current_features:
                current_features.remove(feature)
                print(f"å·²ç§»é™¤ç‰¹å¾: {feature}")
            else:
                print(f"è­¦å‘Š: ç‰¹å¾ {feature} ä¸åœ¨å½“å‰ç‰¹å¾åˆ—è¡¨ä¸­")
            
        # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰ç‰¹å¾å‰©ä½™
        if len(current_features) == 0:
            print("âŒ æ‰€æœ‰ç‰¹å¾éƒ½è¢«ç§»é™¤äº†ï¼")
            break
            
        iteration += 1
        
        # é˜²æ­¢æ— é™å¾ªç¯
        if iteration > 50:
            print("âš ï¸ è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°ï¼Œå¼ºåˆ¶ç»ˆæ­¢")
            break
    
    # æ£€æŸ¥ä¿æŠ¤çš„ç‰¹å¾æ˜¯å¦è¿˜åœ¨æœ€ç»ˆç‰¹å¾é›†ä¸­
    if protected_feature in current_features:
        print(f"\nâœ… ä¿æŠ¤çš„ç‰¹å¾ '{protected_feature}' å·²ä¿ç•™åœ¨æœ€ç»ˆç‰¹å¾é›†ä¸­")
    else:
        print(f"\nâš ï¸ ä¿æŠ¤çš„ç‰¹å¾ '{protected_feature}' æœªåœ¨æœ€ç»ˆç‰¹å¾é›†ä¸­")
    
    # è¾“å‡ºæœ€ç»ˆç»“æœ
    print(f"\nğŸ¯ æœ€ç»ˆé€‰æ‹©çš„ç‰¹å¾ ({len(current_features)}ä¸ª):")
    # æ‹Ÿåˆæœ€ç»ˆæ¨¡å‹
    X_final = data[current_features]
    y_final = data[target_col]
    X_final_with_const = add_constant(X_final)
    final_model = sm.OLS(y_final, X_final_with_const).fit()
    
    # è®¡ç®—æœ€ç»ˆVIFå€¼
    final_vif_data = pd.DataFrame()
    final_vif_data["feature"] = X_final_with_const.columns
    final_vif_data["VIF"] = [variance_inflation_factor(X_final_with_const.values, i) 
                           for i in range(X_final_with_const.shape[1])]
    final_vif_data = final_vif_data[final_vif_data['feature'] != 'const']
    
    for feature in current_features:
        p_value = final_model.pvalues[feature]
        vif_value = final_vif_data[final_vif_data['feature'] == feature]['VIF'].values[0]
        protected_indicator = " (ä¿æŠ¤)" if feature == protected_feature else ""
        print(f"  {feature}: p-value={p_value:.4f}, VIF={vif_value:.2f}{protected_indicator}")
    
    return current_features, final_model


def feature_selection_process3(data, target_col, significance_level=0.05, vif_threshold=10, r2_tolerance=0.02):
    """
    ä¼˜åŒ–ç‰ˆç‰¹å¾é€‰æ‹©æµç¨‹ï¼šåœ¨VIFå’Œæ˜¾è‘—æ€§ç­›é€‰çš„åŒæ—¶ï¼Œæœ€å¤§åŒ–ä¿æŒæ¨¡å‹RÂ²å‡†ç¡®æ€§
    
    å‚æ•°:
    data: åŒ…å«ç‰¹å¾å’Œç›®æ ‡å˜é‡çš„DataFrame
    target_col: ç›®æ ‡å˜é‡åˆ—å
    significance_level: æ˜¾è‘—æ€§æ°´å¹³ï¼Œé»˜è®¤0.05
    vif_threshold: VIFé˜ˆå€¼ï¼Œé»˜è®¤10
    r2_tolerance: RÂ²ä¸‹é™å®¹å¿åº¦ï¼Œé»˜è®¤0.02ï¼ˆ2%ï¼‰
    
    è¿”å›:
    æœ€ç»ˆé€‰æ‹©çš„ç‰¹å¾åˆ—è¡¨å’Œæœ€ç»ˆçš„å›å½’æ¨¡å‹
    """
    
    # 1. ç­›é€‰æ‰åŒ…å«ç‰¹å®šå…³é”®è¯çš„ç‰¹å¾
    exclude_keywords = ['3D', 'è…°å¸¦', 'å‚ç›´', 'å†…éƒ¨', 'èº¯å¹²æ‰­', 'è§’åº¦']
    initial_features = [col for col in data.columns 
                       if col != target_col and 
                       not any(keyword in col for keyword in exclude_keywords)]
    
    print(f"åˆå§‹ç‰¹å¾æ•°é‡: {len(initial_features)}")
    print(f"æ’é™¤çš„ç‰¹å¾: {[col for col in data.columns if col != target_col and col not in initial_features]}")
    print(f"RÂ²å®¹å¿åº¦: {r2_tolerance}")
    
    current_features = initial_features.copy()
    iteration = 1
    best_r2 = 0
    best_features = current_features.copy()
    
    # è®¡ç®—åˆå§‹æ¨¡å‹çš„RÂ²ä½œä¸ºåŸºå‡†
    X_initial = data[initial_features]
    y_initial = data[target_col]
    X_initial_with_const = add_constant(X_initial)
    initial_model = sm.OLS(y_initial, X_initial_with_const).fit()
    baseline_r2 = initial_model.rsquared
    best_r2 = baseline_r2
    print(f"åŸºå‡†æ¨¡å‹RÂ²: {baseline_r2:.4f}")
    
    while True:
        print(f"\n=== ç¬¬ {iteration} æ¬¡è¿­ä»£ ===")
        
        # å‡†å¤‡æ•°æ®
        X = data[current_features]
        y = data[target_col]
        X_with_const = add_constant(X)
        
        # æ‹Ÿåˆå›å½’æ¨¡å‹
        try:
            model = sm.OLS(y, X_with_const).fit()
            current_r2 = model.rsquared
            current_adj_r2 = model.rsquared_adj
        except Exception as e:
            print(f"å›å½’æ¨¡å‹æ‹Ÿåˆå¤±è´¥: {e}")
            break
            
        # 2. æ£€æŸ¥è‡ªå˜é‡æ˜¾è‘—æ€§
        pvalues = model.pvalues.drop('const')
        insignificant_features = pvalues[pvalues > significance_level].index.tolist()
        
        # 3. è®¡ç®—VIFå€¼
        vif_data = pd.DataFrame()
        vif_data["feature"] = X_with_const.columns
        vif_data["VIF"] = [variance_inflation_factor(X_with_const.values, i) 
                          for i in range(X_with_const.shape[1])]
        
        vif_data = vif_data[vif_data['feature'] != 'const']
        high_vif_features = vif_data[vif_data["VIF"] > vif_threshold]
        
        print(f"å½“å‰ç‰¹å¾æ•°é‡: {len(current_features)}")
        print(f"å½“å‰RÂ²: {current_r2:.4f}, è°ƒæ•´åRÂ²: {current_adj_r2:.4f}")
        print(f"ä¸æ˜¾è‘—çš„ç‰¹å¾ (p > {significance_level}): {insignificant_features}")
        print(f"VIF > {vif_threshold} çš„ç‰¹å¾:")
        print(high_vif_features)
        
        # æ£€æŸ¥ç»ˆæ­¢æ¡ä»¶
        if len(insignificant_features) == 0 and len(high_vif_features) == 0:
            print("\nâœ… è¾¾åˆ°ç»ˆæ­¢æ¡ä»¶: æ‰€æœ‰ç‰¹å¾éƒ½æ˜¾è‘—ä¸”VIF < 10")
            break
            
        # è®°å½•å½“å‰æœ€ä½³æ¨¡å‹ï¼ˆå¦‚æœRÂ²ä¸‹é™åœ¨å®¹å¿èŒƒå›´å†…ä¸”æ»¡è¶³å…¶ä»–æ¡ä»¶ï¼‰
        r2_drop = baseline_r2 - current_r2
        if (len(insignificant_features) == 0 and 
            r2_drop <= r2_tolerance and 
            current_r2 > best_r2 - 0.01):  # å…è®¸è½»å¾®ä¸‹é™
            best_r2 = current_r2
            best_features = current_features.copy()
            print(f"æ›´æ–°æœ€ä½³æ¨¡å‹: RÂ² = {current_r2:.4f}, ç‰¹å¾æ•° = {len(current_features)}")
        
        # ç”Ÿæˆå€™é€‰ç§»é™¤ç‰¹å¾åˆ—è¡¨ï¼ˆè€ƒè™‘å¯¹RÂ²çš„å½±å“ï¼‰
        candidate_features_to_remove = []
        
        # ä¼˜å…ˆè€ƒè™‘ä¸æ˜¾è‘—çš„ç‰¹å¾
        if len(insignificant_features) > 0:
            # å¯¹ä¸æ˜¾è‘—ç‰¹å¾æŒ‰på€¼æ’åºï¼Œå¹¶è¯„ä¼°ç§»é™¤å¯¹RÂ²çš„å½±å“
            for feature in sorted(insignificant_features, key=lambda x: pvalues[x], reverse=True):
                candidate_features_to_remove.append({
                    'feature': feature,
                    'reason': 'insignificant',
                    'p_value': pvalues[feature],
                    'vif': vif_data[vif_data['feature'] == feature]['VIF'].values[0] if feature in vif_data['feature'].values else 0
                })
        
        # ç„¶åè€ƒè™‘é«˜VIFç‰¹å¾
        if len(high_vif_features) > 0:
            for _, row in high_vif_features.sort_values('VIF', ascending=False).iterrows():
                candidate_features_to_remove.append({
                    'feature': row['feature'],
                    'reason': 'high_vif',
                    'p_value': pvalues[row['feature']] if row['feature'] in pvalues else 1.0,
                    'vif': row['VIF']
                })
        
        # æ™ºèƒ½é€‰æ‹©è¦ç§»é™¤çš„ç‰¹å¾
        features_to_remove = []
        
        if candidate_features_to_remove:
            # è¯„ä¼°ç§»é™¤æ¯ä¸ªå€™é€‰ç‰¹å¾å¯¹RÂ²çš„å½±å“
            removal_impact = []
            for candidate in candidate_features_to_remove:
                feature = candidate['feature']
                # æµ‹è¯•ç§»é™¤è¯¥ç‰¹å¾åçš„RÂ²
                test_features = [f for f in current_features if f != feature]
                if test_features:  # ç¡®ä¿è¿˜æœ‰ç‰¹å¾å‰©ä½™
                    X_test = data[test_features]
                    X_test_with_const = add_constant(X_test)
                    try:
                        test_model = sm.OLS(y, X_test_with_const).fit()
                        r2_after_removal = test_model.rsquared
                        r2_drop = current_r2 - r2_after_removal
                    except:
                        r2_drop = float('inf')  # å¦‚æœæ‹Ÿåˆå¤±è´¥ï¼Œè®¤ä¸ºå½±å“å¾ˆå¤§
                else:
                    r2_drop = float('inf')
                
                removal_impact.append({
                    'feature': feature,
                    'reason': candidate['reason'],
                    'p_value': candidate['p_value'],
                    'vif': candidate['vif'],
                    'r2_drop': r2_drop
                })
            
            # é€‰æ‹©å¯¹RÂ²å½±å“æœ€å°çš„ç‰¹å¾è¿›è¡Œç§»é™¤
            if removal_impact:
                # ä¼˜å…ˆé€‰æ‹©å¯¹RÂ²å½±å“å°ä¸”ä¸æ˜¾è‘—çš„ç‰¹å¾
                low_impact_insignificant = [x for x in removal_impact if x['reason'] == 'insignificant' and x['r2_drop'] <= r2_tolerance]
                if low_impact_insignificant:
                    best_candidate = min(low_impact_insignificant, key=lambda x: x['r2_drop'])
                else:
                    # å¦‚æœæ²¡æœ‰å¯¹RÂ²å½±å“å°çš„ä¸æ˜¾è‘—ç‰¹å¾ï¼Œé€‰æ‹©å¯¹RÂ²å½±å“å°çš„é«˜VIFç‰¹å¾
                    low_impact_high_vif = [x for x in removal_impact if x['reason'] == 'high_vif' and x['r2_drop'] <= r2_tolerance]
                    if low_impact_high_vif:
                        best_candidate = min(low_impact_high_vif, key=lambda x: x['r2_drop'])
                    else:
                        # å¦‚æœæ‰€æœ‰ç§»é™¤éƒ½ä¼šå¯¼è‡´RÂ²å¤§å¹…ä¸‹é™ï¼Œé€‰æ‹©å½±å“ç›¸å¯¹æœ€å°çš„
                        best_candidate = min(removal_impact, key=lambda x: x['r2_drop'])
                
                features_to_remove.append(best_candidate['feature'])
                print(f"ç§»é™¤ç‰¹å¾: {best_candidate['feature']} ({best_candidate['reason']})")
                print(f"  p-value: {best_candidate['p_value']:.4f}, VIF: {best_candidate['vif']:.2f}, RÂ²ä¸‹é™: {best_candidate['r2_drop']:.4f}")
        
        # å¦‚æœæ²¡æœ‰åˆé€‚çš„å€™é€‰ç‰¹å¾ï¼ˆæ‰€æœ‰ç§»é™¤éƒ½ä¼šå¯¼è‡´RÂ²å¤§å¹…ä¸‹é™ï¼‰
        if not features_to_remove:
            print("âš ï¸ æ²¡æœ‰åˆé€‚çš„ç‰¹å¾å¯ä»¥ç§»é™¤ï¼ˆæ‰€æœ‰ç§»é™¤éƒ½ä¼šå¯¼è‡´RÂ²å¤§å¹…ä¸‹é™ï¼‰")
            # æ£€æŸ¥æ˜¯å¦æ»¡è¶³æ”¾å®½çš„æ¡ä»¶
            if len(insignificant_features) == 0:
                print("âœ… æ‰€æœ‰ç‰¹å¾éƒ½æ˜¾è‘—ï¼Œæ¥å—å½“å‰æ¨¡å‹")
                break
            else:
                # å¼ºåˆ¶ç§»é™¤på€¼æœ€å¤§çš„ç‰¹å¾
                if insignificant_features:
                    max_p_feature = max(insignificant_features, key=lambda x: pvalues[x])
                    features_to_remove.append(max_p_feature)
                    print(f"å¼ºåˆ¶ç§»é™¤æœ€ä¸æ˜¾è‘—ç‰¹å¾: {max_p_feature}")
        
        # ç§»é™¤ç‰¹å¾
        for feature in features_to_remove:
            if feature in current_features:
                current_features.remove(feature)
                print(f"å·²ç§»é™¤ç‰¹å¾: {feature}")
            
        # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰ç‰¹å¾å‰©ä½™
        if len(current_features) == 0:
            print("âŒ æ‰€æœ‰ç‰¹å¾éƒ½è¢«ç§»é™¤äº†ï¼")
            current_features = best_features  # æ¢å¤åˆ°æœ€ä½³ç‰¹å¾é›†
            break
            
        iteration += 1
        
        # é˜²æ­¢æ— é™å¾ªç¯
        if iteration > 30:
            print("âš ï¸ è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°ï¼Œå¼ºåˆ¶ç»ˆæ­¢")
            break
    
    # æœ€ç»ˆæ¨¡å‹è¯„ä¼°
    print(f"\nğŸ¯ æœ€ç»ˆé€‰æ‹©çš„ç‰¹å¾ ({len(current_features)}ä¸ª):")
    X_final = data[current_features]
    y_final = data[target_col]
    X_final_with_const = add_constant(X_final)
    final_model = sm.OLS(y_final, X_final_with_const).fit()
    
    # è®¡ç®—æœ€ç»ˆæŒ‡æ ‡
    final_vif_data = pd.DataFrame()
    final_vif_data["feature"] = X_final_with_const.columns
    final_vif_data["VIF"] = [variance_inflation_factor(X_final_with_const.values, i) 
                           for i in range(X_final_with_const.shape[1])]
    final_vif_data = final_vif_data[final_vif_data['feature'] != 'const']
    
    final_r2 = final_model.rsquared
    final_adj_r2 = final_model.rsquared_adj
    r2_reduction = baseline_r2 - final_r2
    
    print(f"\nğŸ“Š æ¨¡å‹æ€§èƒ½æ€»ç»“:")
    print(f"åˆå§‹RÂ²: {baseline_r2:.4f}")
    print(f"æœ€ç»ˆRÂ²: {final_r2:.4f}")
    print(f"RÂ²ä¸‹é™: {r2_reduction:.4f} ({r2_reduction/baseline_r2*100:.1f}%)")
    print(f"è°ƒæ•´åRÂ²: {final_adj_r2:.4f}")
    
    for feature in current_features:
        p_value = final_model.pvalues[feature]
        vif_value = final_vif_data[final_vif_data['feature'] == feature]['VIF'].values[0]
        significance_indicator = " âœ“" if p_value <= significance_level else " âœ—"
        vif_indicator = " âœ“" if vif_value <= vif_threshold else " âœ—"
        print(f"  {feature}: p-value={p_value:.4f}{significance_indicator}, VIF={vif_value:.2f}{vif_indicator}")
    
    return current_features, final_model
"""

def feature_selection_process4(data, target_col, significance_level=0.05, vif_threshold=10, r2_tolerance=0.02):
    """
    ä¼˜åŒ–ç‰ˆç‰¹å¾é€‰æ‹©æµç¨‹ï¼šåœ¨VIFå’Œæ˜¾è‘—æ€§ç­›é€‰çš„åŒæ—¶ï¼Œæœ€å¤§åŒ–ä¿æŒæ¨¡å‹RÂ²å‡†ç¡®æ€§
    ä¼˜å…ˆç§»é™¤å¯¹æ¨¡å‹æ€§èƒ½å½±å“æœ€å°çš„ç‰¹å¾
    
    å‚æ•°:
    data: åŒ…å«ç‰¹å¾å’Œç›®æ ‡å˜é‡çš„DataFrame
    target_col: ç›®æ ‡å˜é‡åˆ—å
    significance_level: æ˜¾è‘—æ€§æ°´å¹³ï¼Œé»˜è®¤0.05
    vif_threshold: VIFé˜ˆå€¼ï¼Œé»˜è®¤10
    r2_tolerance: RÂ²ä¸‹é™å®¹å¿åº¦ï¼Œé»˜è®¤0.02ï¼ˆ2%ï¼‰
    
    è¿”å›:
    æœ€ç»ˆé€‰æ‹©çš„ç‰¹å¾åˆ—è¡¨å’Œæœ€ç»ˆçš„å›å½’æ¨¡å‹
    """
    
    # 1. ç­›é€‰æ‰åŒ…å«ç‰¹å®šå…³é”®è¯çš„ç‰¹å¾
    exclude_keywords = ['3D', 'è…°å¸¦', 'å‚ç›´', 'å†…éƒ¨', 'èº¯å¹²æ‰­', 'è§’åº¦']
    initial_features = [col for col in data.columns 
                       if col != target_col and 
                       not any(keyword in col for keyword in exclude_keywords)]
    
    print(f"åˆå§‹ç‰¹å¾æ•°é‡: {len(initial_features)}")
    print(f"æ’é™¤çš„ç‰¹å¾: {[col for col in data.columns if col != target_col and col not in initial_features]}")
    print(f"RÂ²å®¹å¿åº¦: {r2_tolerance}")
    
    current_features = initial_features.copy()
    iteration = 1
    best_r2 = 0
    best_features = current_features.copy()
    
    # è®¡ç®—åˆå§‹æ¨¡å‹çš„RÂ²ä½œä¸ºåŸºå‡†
    X_initial = data[initial_features]
    y_initial = data[target_col]
    X_initial_with_const = add_constant(X_initial)
    initial_model = sm.OLS(y_initial, X_initial_with_const).fit()
    baseline_r2 = initial_model.rsquared
    best_r2 = baseline_r2
    print(f"åŸºå‡†æ¨¡å‹RÂ²: {baseline_r2:.4f}")
    
    while True:
        print(f"\n=== ç¬¬ {iteration} æ¬¡è¿­ä»£ ===")
        
        # å‡†å¤‡æ•°æ®
        X = data[current_features]
        y = data[target_col]
        X_with_const = add_constant(X)
        
        # æ‹Ÿåˆå›å½’æ¨¡å‹
        try:
            model = sm.OLS(y, X_with_const).fit()
            current_r2 = model.rsquared
            current_adj_r2 = model.rsquared_adj
        except Exception as e:
            print(f"å›å½’æ¨¡å‹æ‹Ÿåˆå¤±è´¥: {e}")
            break
            
        # 2. æ£€æŸ¥è‡ªå˜é‡æ˜¾è‘—æ€§
        pvalues = model.pvalues.drop('const')
        insignificant_features = pvalues[pvalues > significance_level].index.tolist()
        
        # 3. è®¡ç®—VIFå€¼
        vif_data = pd.DataFrame()
        vif_data["feature"] = X_with_const.columns
        vif_data["VIF"] = [variance_inflation_factor(X_with_const.values, i) 
                          for i in range(X_with_const.shape[1])]
        
        vif_data = vif_data[vif_data['feature'] != 'const']
        high_vif_features = vif_data[vif_data["VIF"] > vif_threshold]
        
        print(f"å½“å‰ç‰¹å¾æ•°é‡: {len(current_features)}")
        print(f"å½“å‰RÂ²: {current_r2:.4f}, è°ƒæ•´åRÂ²: {current_adj_r2:.4f}")
        print(f"ä¸æ˜¾è‘—çš„ç‰¹å¾ (p > {significance_level}): {insignificant_features}")
        print(f"VIF > {vif_threshold} çš„ç‰¹å¾:")
        print(high_vif_features)
        
        # æ£€æŸ¥ç»ˆæ­¢æ¡ä»¶
        if len(insignificant_features) == 0 and len(high_vif_features) == 0:
            print("\nâœ… è¾¾åˆ°ç»ˆæ­¢æ¡ä»¶: æ‰€æœ‰ç‰¹å¾éƒ½æ˜¾è‘—ä¸”VIF < 10")
            break
            
        # è®°å½•å½“å‰æœ€ä½³æ¨¡å‹ï¼ˆå¦‚æœRÂ²ä¸‹é™åœ¨å®¹å¿èŒƒå›´å†…ä¸”æ»¡è¶³å…¶ä»–æ¡ä»¶ï¼‰
        r2_drop = baseline_r2 - current_r2
        if (len(insignificant_features) == 0 and 
            r2_drop <= r2_tolerance and 
            current_r2 > best_r2 - 0.01):  # å…è®¸è½»å¾®ä¸‹é™
            best_r2 = current_r2
            best_features = current_features.copy()
            print(f"æ›´æ–°æœ€ä½³æ¨¡å‹: RÂ² = {current_r2:.4f}, ç‰¹å¾æ•° = {len(current_features)}")
        
        # ç”Ÿæˆæ‰€æœ‰å€™é€‰ç§»é™¤ç‰¹å¾åˆ—è¡¨ï¼ˆåŒ…æ‹¬ä¸æ˜¾è‘—å’Œé«˜VIFç‰¹å¾ï¼‰
        candidate_features_to_remove = []
        
        # æ”¶é›†æ‰€æœ‰éœ€è¦ç§»é™¤çš„å€™é€‰ç‰¹å¾ï¼ˆä¸åŒºåˆ†ä¼˜å…ˆçº§ï¼‰
        if len(insignificant_features) > 0:
            for feature in insignificant_features:
                candidate_features_to_remove.append({
                    'feature': feature,
                    'reason': 'insignificant',
                    'p_value': pvalues[feature],
                    'vif': vif_data[vif_data['feature'] == feature]['VIF'].values[0] if feature in vif_data['feature'].values else 0
                })
        
        if len(high_vif_features) > 0:
            for _, row in high_vif_features.iterrows():
                # é¿å…é‡å¤æ·»åŠ ï¼ˆå¦‚æœæŸä¸ªç‰¹å¾æ—¢é«˜VIFåˆä¸æ˜¾è‘—ï¼‰
                if not any(candidate['feature'] == row['feature'] for candidate in candidate_features_to_remove):
                    candidate_features_to_remove.append({
                        'feature': row['feature'],
                        'reason': 'high_vif',
                        'p_value': pvalues[row['feature']] if row['feature'] in pvalues else 1.0,
                        'vif': row['VIF']
                    })
        
        # æ™ºèƒ½é€‰æ‹©è¦ç§»é™¤çš„ç‰¹å¾ - åŸºäºå¯¹RÂ²çš„å½±å“
        features_to_remove = []
        
        if candidate_features_to_remove:
            # è¯„ä¼°ç§»é™¤æ¯ä¸ªå€™é€‰ç‰¹å¾å¯¹RÂ²çš„å½±å“
            removal_impact = []
            for candidate in candidate_features_to_remove:
                feature = candidate['feature']
                # æµ‹è¯•ç§»é™¤è¯¥ç‰¹å¾åçš„RÂ²
                test_features = [f for f in current_features if f != feature]
                if test_features:  # ç¡®ä¿è¿˜æœ‰ç‰¹å¾å‰©ä½™
                    X_test = data[test_features]
                    X_test_with_const = add_constant(X_test)
                    try:
                        test_model = sm.OLS(y, X_test_with_const).fit()
                        r2_after_removal = test_model.rsquared
                        r2_drop = current_r2 - r2_after_removal
                    except:
                        r2_drop = float('inf')  # å¦‚æœæ‹Ÿåˆå¤±è´¥ï¼Œè®¤ä¸ºå½±å“å¾ˆå¤§
                else:
                    r2_drop = float('inf')
                
                removal_impact.append({
                    'feature': feature,
                    'reason': candidate['reason'],
                    'p_value': candidate['p_value'],
                    'vif': candidate['vif'],
                    'r2_drop': r2_drop
                })
            
            # æŒ‰RÂ²å½±å“ä»å°åˆ°å¤§æ’åº
            removal_impact_sorted = sorted(removal_impact, key=lambda x: x['r2_drop'])
            
            # é€‰æ‹©å¯¹RÂ²å½±å“æœ€å°çš„ç‰¹å¾è¿›è¡Œç§»é™¤
            best_candidate = None
            
            # é¦–å…ˆå¯»æ‰¾åœ¨å®¹å¿åº¦èŒƒå›´å†…çš„å½±å“æœ€å°çš„ç‰¹å¾
            for candidate in removal_impact_sorted:
                if candidate['r2_drop'] <= r2_tolerance:
                    best_candidate = candidate
                    break
            
            # å¦‚æœæ²¡æœ‰åœ¨å®¹å¿åº¦èŒƒå›´å†…çš„ç‰¹å¾ï¼Œé€‰æ‹©å½±å“ç›¸å¯¹æœ€å°çš„
            if best_candidate is None and removal_impact_sorted:
                best_candidate = removal_impact_sorted[0]
                print(f"âš ï¸ æ‰€æœ‰ç§»é™¤éƒ½ä¼šå¯¼è‡´RÂ²ä¸‹é™è¶…è¿‡å®¹å¿åº¦ï¼Œé€‰æ‹©å½±å“æœ€å°çš„")
            
            if best_candidate:
                features_to_remove.append(best_candidate['feature'])
                print(f"ç§»é™¤ç‰¹å¾: {best_candidate['feature']} ({best_candidate['reason']})")
                print(f"  p-value: {best_candidate['p_value']:.4f}, VIF: {best_candidate['vif']:.2f}, RÂ²ä¸‹é™: {best_candidate['r2_drop']:.4f}")
                
                # æ˜¾ç¤ºå…¶ä»–å€™é€‰ç‰¹å¾çš„RÂ²å½±å“ï¼ˆç”¨äºè°ƒè¯•ï¼‰
                if len(removal_impact_sorted) > 1:
                    print(f"  å…¶ä»–å€™é€‰ç‰¹å¾RÂ²å½±å“:")
                    for i, candidate in enumerate(removal_impact_sorted[1:4]):  # æ˜¾ç¤ºå‰å‡ ä¸ª
                        print(f"    {candidate['feature']}: RÂ²ä¸‹é™ {candidate['r2_drop']:.4f} ({candidate['reason']})")
        
        # å¦‚æœæ²¡æœ‰åˆé€‚çš„å€™é€‰ç‰¹å¾ï¼ˆæ‰€æœ‰ç§»é™¤éƒ½ä¼šå¯¼è‡´RÂ²å¤§å¹…ä¸‹é™ï¼‰
        if not features_to_remove:
            print("âš ï¸ æ²¡æœ‰åˆé€‚çš„ç‰¹å¾å¯ä»¥ç§»é™¤ï¼ˆæ‰€æœ‰ç§»é™¤éƒ½ä¼šå¯¼è‡´RÂ²å¤§å¹…ä¸‹é™ï¼‰")
            # æ£€æŸ¥æ˜¯å¦æ»¡è¶³æ”¾å®½çš„æ¡ä»¶
            if len(insignificant_features) == 0:
                print("âœ… æ‰€æœ‰ç‰¹å¾éƒ½æ˜¾è‘—ï¼Œæ¥å—å½“å‰æ¨¡å‹")
                break
            else:
                # å¼ºåˆ¶ç§»é™¤på€¼æœ€å¤§çš„ç‰¹å¾
                if insignificant_features:
                    max_p_feature = max(insignificant_features, key=lambda x: pvalues[x])
                    features_to_remove.append(max_p_feature)
                    print(f"å¼ºåˆ¶ç§»é™¤æœ€ä¸æ˜¾è‘—ç‰¹å¾: {max_p_feature}")
        
        # ç§»é™¤ç‰¹å¾
        for feature in features_to_remove:
            if feature in current_features:
                current_features.remove(feature)
                print(f"å·²ç§»é™¤ç‰¹å¾: {feature}")
            
        # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰ç‰¹å¾å‰©ä½™
        if len(current_features) == 0:
            print("âŒ æ‰€æœ‰ç‰¹å¾éƒ½è¢«ç§»é™¤äº†ï¼")
            current_features = best_features  # æ¢å¤åˆ°æœ€ä½³ç‰¹å¾é›†
            break
            
        iteration += 1
        
        # é˜²æ­¢æ— é™å¾ªç¯
        if iteration > 30:
            print("âš ï¸ è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°ï¼Œå¼ºåˆ¶ç»ˆæ­¢")
            break
    
    # æœ€ç»ˆæ¨¡å‹è¯„ä¼°
    print(f"\nğŸ¯ æœ€ç»ˆé€‰æ‹©çš„ç‰¹å¾ ({len(current_features)}ä¸ª):")
    X_final = data[current_features]
    y_final = data[target_col]
    X_final_with_const = add_constant(X_final)
    final_model = sm.OLS(y_final, X_final_with_const).fit()
    
    # è®¡ç®—æœ€ç»ˆæŒ‡æ ‡
    final_vif_data = pd.DataFrame()
    final_vif_data["feature"] = X_final_with_const.columns
    final_vif_data["VIF"] = [variance_inflation_factor(X_final_with_const.values, i) 
                           for i in range(X_final_with_const.shape[1])]
    final_vif_data = final_vif_data[final_vif_data['feature'] != 'const']
    
    final_r2 = final_model.rsquared
    final_adj_r2 = final_model.rsquared_adj
    r2_reduction = baseline_r2 - final_r2
    
    print(f"\nğŸ“Š æ¨¡å‹æ€§èƒ½æ€»ç»“:")
    print(f"åˆå§‹RÂ²: {baseline_r2:.4f}")
    print(f"æœ€ç»ˆRÂ²: {final_r2:.4f}")
    print(f"RÂ²ä¸‹é™: {r2_reduction:.4f} ({r2_reduction/baseline_r2*100:.1f}%)")
    print(f"è°ƒæ•´åRÂ²: {final_adj_r2:.4f}")
    
    for feature in current_features:
        p_value = final_model.pvalues[feature]
        vif_value = final_vif_data[final_vif_data['feature'] == feature]['VIF'].values[0]
        significance_indicator = " âœ“" if p_value <= significance_level else " âœ—"
        vif_indicator = " âœ“" if vif_value <= vif_threshold else " âœ—"
        print(f"  {feature}: p-value={p_value:.4f}{significance_indicator}, VIF={vif_value:.2f}{vif_indicator}")
    
    return current_features, final_model

# è®¡ç®—VIF
def calculate_vif(data, include_const=True, threshold=10):
    """
    è®¡ç®—æ•°æ®é›†ä¸­æ¯ä¸ªå˜é‡çš„VIFå€¼
    
    å‚æ•°:
    data: pandas DataFrame, è¾“å…¥æ•°æ®
    include_const: bool, æ˜¯å¦åœ¨ç»“æœä¸­åŒ…å«å¸¸æ•°é¡¹
    threshold: float, VIFé˜ˆå€¼ï¼Œç”¨äºæ ‡è®°é«˜å¤šé‡å…±çº¿æ€§
    
    è¿”å›:
    pandas DataFrame, åŒ…å«å˜é‡åå’Œå¯¹åº”çš„VIFå€¼
    """
    # è¾“å…¥éªŒè¯
    if not isinstance(data, pd.DataFrame):
        raise ValueError("è¾“å…¥æ•°æ®å¿…é¡»æ˜¯pandas DataFrame")
    
    if data.empty:
        raise ValueError("è¾“å…¥æ•°æ®ä¸èƒ½ä¸ºç©º")
    
    # æ£€æŸ¥æ˜¯å¦æœ‰ç¼ºå¤±å€¼
    if data.isnull().any().any():
        raise ValueError("æ•°æ®ä¸­åŒ…å«ç¼ºå¤±å€¼ï¼Œè¯·å…ˆå¤„ç†ç¼ºå¤±å€¼")
    
    # æ·»åŠ å¸¸æ•°é¡¹
    X = add_constant(data)
    
    # è®¡ç®—æ¯ä¸ªå˜é‡çš„VIF
    vif_data = pd.DataFrame()
    vif_data['Variable'] = X.columns
    vif_data['VIF'] = [variance_inflation_factor(X.values, i) 
                      for i in range(X.shape[1])]
    
    # æ·»åŠ è¯Šæ–­ä¿¡æ¯
    vif_data['High_VIF'] = vif_data['VIF'] > threshold
    
    # æ˜¯å¦æ’é™¤å¸¸æ•°é¡¹
    if not include_const:
        vif_data = vif_data[vif_data['Variable'] != 'const']
    
    # æŒ‰VIFå€¼æ’åº
    vif_data = vif_data.sort_values('VIF', ascending=False)
    
    return vif_data.reset_index(drop=True)

##  --------------------------------  VAT  --------------------------

VAT_feature = AA_20240105_All_models['VAT_forward_linear_model']['Selected_feature'] ## é€æ­¥å›å½’æ¨¡å‹çš„è‡ªå˜é‡
X_VAT_train = X_train[VAT_feature]  ## è‡ªå˜é‡å€¼
Y_VAT_train = y_train["VATmass"]
Y_VAT_train = Y_VAT_train.to_frame().rename(columns={'VATmass': 'VATmass'})

# åˆå¹¶Xå’ŒY
VAT_train_data = pd.concat([X_VAT_train, Y_VAT_train], axis=1)

## ä¸ºäº†å¯é‡å¤ï¼Œè¿›è¡ŒVIFå’Œæ˜¾è‘—æ€§ç­›é€‰
# ä¿ç•™ ["æ ‡å‡†ç«™å§¿è…°å›´", "å¤§è…¿å›´åº¦å·¦è¾¹.æ°´å¹³çº¿.", "å¯¹é«˜è‡€éƒ¨çš„è…°éƒ¨åé¢",   "è‡‚é•¿å³", "ä¸Šè‡‚å›´åº¦å·¦è¾¹", "age"]

VAT_targets, VAT_forward_linear_model = feature_selection_process4(data = VAT_train_data, target_col= "VATmass", significance_level=0.05, vif_threshold=10, r2_tolerance=0.01)

X_VAT_train_Update = X_train[VAT_targets]
vif_VAT_result_Update = calculate_vif(X_VAT_train_Update, include_const= False)  ## è®¡ç®—è‡ªå˜é‡çš„VIF
print(vif_VAT_result_Update)

vif_VAT_result_Update.to_csv("C:/Users/zjl__/Desktop/output_zhaojialu/output/ForwardRegression2/VIF_VAT_params.csv", index=False)

# è®¡ç®—DWç»Ÿè®¡é‡
dw_VAT = durbin_watson(VAT_forward_linear_model.resid)
print(f"VAT-Durbin-Watsonç»Ÿè®¡é‡: {dw_VAT:.4f}") # VAT-Durbin-Watsonç»Ÿè®¡é‡: 1.8993

##  --------------------------------  FM  --------------------------

FM_feature = AA_20240105_All_models['FM_forward_linear_model']['Selected_feature']
X_FM_train = X_train[FM_feature]  ## è‡ªå˜é‡å€¼
Y_FM_train = y_train["FM"]
Y_FM_train = Y_FM_train.to_frame().rename(columns={'FM': 'FM'})

# åˆå¹¶Xå’ŒY
FM_train_data = pd.concat([X_FM_train, Y_FM_train], axis=1)

# ä¸ºäº†å¯é‡å¤ï¼Œå¹¶ç»“åˆ VIF < 10 å’Œ å›å½’æ˜¾è‘—æ€§
# ä¿ç•™ [ "è†å›´åº¦å·¦è¾¹", "gender", "è„šè¸å›´å·¦è¾¹", "å‰è‡‚å›´åº¦å³è¾¹","æ ‡å‡†ç«™å§¿è…°å›´", "age", "èƒ¯éƒ¨é•¿åº¦", "å¤´é«˜", ,'è…¿è‚šå›´åº¦å·¦è¾¹', "æœ€å°è…¿å›´åº¦å·¦è¾¹"]

FM_targets, FM_forward_linear_model = feature_selection_process4(data = FM_train_data, target_col= "FM", significance_level=0.05, vif_threshold=10, r2_tolerance=0.01)

X_FM_train_Update = X_train[FM_targets]
vif_FM_result_Update = calculate_vif(X_FM_train_Update, include_const= False)  ## è®¡ç®—è‡ªå˜é‡çš„VIF
vif_FM_result_Update.to_csv("C:/Users/zjl__/Desktop/output_zhaojialu/output/ForwardRegression2/VIF_FM_params.csv", index=False)
print(vif_FM_result_Update)

dw_FM = durbin_watson(FM_forward_linear_model.resid)
print(f"FM-Durbin-Watsonç»Ÿè®¡é‡: {dw_FM:.4f}") # FM-Durbin-Watsonç»Ÿè®¡é‡: 1.9210

## -------------------------------- FMI ----------------------------------

FMI_feature = AA_20240105_All_models['FMI_forward_linear_model']['Selected_feature']
X_FMI_train = X_train[FMI_feature]  ## è‡ªå˜é‡å€¼
Y_FMI_train = y_train["FMI"]
Y_FMI_train = Y_FMI_train.to_frame().rename(columns={'FMI': 'FMI'})

# åˆå¹¶Xå’ŒY
FMI_train_data = pd.concat([X_FMI_train, Y_FMI_train], axis=1)

# ä¸ºäº†å¯é‡å¤ï¼Œå¹¶ç»“åˆ VIF < 10 å’Œ å›å½’æ˜¾è‘—æ€§
# ä¿ç•™ ["ä¸­è‡€å›´", "èº«ä½“é«˜åº¦", "è†å›´åº¦å·¦è¾¹", "gender",  "age",  "è„šè¸å›´å³è¾¹", "è‡‚é•¿å·¦", "ä¸Šè‡‚å›´åº¦å³è¾¹"]

FMI_targets, FMI_forward_linear_model = feature_selection_process4(data = FMI_train_data, target_col= "FMI", significance_level=0.05, vif_threshold=10, r2_tolerance=0.01)

X_FMI_train_Update = X_train[FMI_targets]
vif_FMI_result_Update = calculate_vif(X_FMI_train_Update, include_const= False)  ## è®¡ç®—è‡ªå˜é‡çš„VIF
vif_FMI_result_Update.to_csv("C:/Users/zjl__/Desktop/output_zhaojialu/output/ForwardRegression2/VIF_FMI_params.csv", index=False)
print(vif_FMI_result_Update)

dw_FMI = durbin_watson(FMI_forward_linear_model.resid)
print(f"FMI-Durbin-Watsonç»Ÿè®¡é‡: {dw_FMI:.4f}") # FMI-Durbin-Watsonç»Ÿè®¡é‡: 1.9920

##  --------------------------------  LM  --------------------------

LM_feature = AA_20240105_All_models['LM_forward_linear_model']['Selected_feature']
X_LM_train = X_train[LM_feature]  ## è‡ªå˜é‡å€¼
Y_LM_train = y_train["LM"]
Y_LM_train = Y_LM_train.to_frame().rename(columns={'LM': 'LM'})

# åˆå¹¶Xå’ŒY
LM_train_data = pd.concat([X_LM_train, Y_LM_train], axis=1)

# ä¸ºäº†å¯é‡å¤ï¼Œå¹¶ç»“åˆ VIF < 10 å’Œ å›å½’æ˜¾è‘—æ€§
# ä¿ç•™ ["gender", "é«˜è‡€éƒ¨å›´åº¦", "è†å›´åº¦å·¦è¾¹", "è„šè¸å›´å·¦è¾¹",  "å‰è‡‚å›´åº¦å³è¾¹",  "èº«ä½“é«˜åº¦", "è…¿è‚šå›´åº¦å·¦è¾¹", "å®½åº¦è…‹çª", "æœ€å°è…¿å›´åº¦å·¦è¾¹"]

LM_targets, LM_forward_linear_model = feature_selection_process4(data = LM_train_data, target_col= "LM", significance_level=0.05, vif_threshold=10, r2_tolerance=0.01)

X_LM_train_Update = X_train[LM_targets]
vif_LM_result_Update = calculate_vif(X_LM_train_Update, include_const= False)  ## è®¡ç®—è‡ªå˜é‡çš„VIF
vif_LM_result_Update.to_csv("C:/Users/zjl__/Desktop/output_zhaojialu/output/ForwardRegression2/VIF_LM_params.csv", index=False)
print(vif_LM_result_Update)

dw_LM = durbin_watson(LM_forward_linear_model.resid)
print(f"LM-Durbin-Watsonç»Ÿè®¡é‡: {dw_LM:.4f}") # LM-Durbin-Watsonç»Ÿè®¡é‡: 2.0129


##  --------------------------------  Android  --------------------------

Android_feature = AA_20240105_All_models['Android_forward_linear_model']['Selected_feature']
X_Android_train = X_train[Android_feature]  ## è‡ªå˜é‡å€¼
Y_Android_train = y_train["Android"]
Y_Android_train = Y_Android_train.to_frame().rename(columns={'Android': 'Android'})

# åˆå¹¶Xå’ŒY
Android_train_data = pd.concat([X_Android_train, Y_Android_train], axis=1)

# ä¸ºäº†å¯é‡å¤ï¼Œå¹¶ç»“åˆ VIF < 10 å’Œ å›å½’æ˜¾è‘—æ€§
# ä¿ç•™ ["æ ‡å‡†ç«™å§¿è…°å›´",  "gender", "å‰è‡‚å›´åº¦å³è¾¹", "è„šè¸å›´å·¦è¾¹",   "è‡€éƒ¨.å¤§è…¿å›´åº¦",  "è‡‚é•¿å·¦",  "è†å›´åº¦å·¦è¾¹", "è…°éƒ¨å¯¹è‡€éƒ¨é«˜åº¦å·¦è¾¹"]

Android_targets, Android_forward_linear_model = feature_selection_process4(data = Android_train_data, target_col= "Android", significance_level=0.05, vif_threshold=10, r2_tolerance=0.01)

X_Android_train_Update = X_train[Android_targets]
vif_Android_result_Update = calculate_vif(X_Android_train_Update, include_const= False)  ## è®¡ç®—è‡ªå˜é‡çš„VIF
vif_Android_result_Update.to_csv("C:/Users/zjl__/Desktop/output_zhaojialu/output/ForwardRegression2/VIF_Android_params.csv", index=False)
print(vif_Android_result_Update)

dw_Android = durbin_watson(Android_forward_linear_model.resid)
print(f"Android-Durbin-Watsonç»Ÿè®¡é‡: {dw_Android:.4f}")   ## Android-Durbin-Watsonç»Ÿè®¡é‡: 1.9673

##  --------------------------------  Gynoid  --------------------------

Gynoid_feature = AA_20240105_All_models['Gynoid_forward_linear_model']['Selected_feature']
X_Gynoid_train = X_train[Gynoid_feature]  ## è‡ªå˜é‡å€¼
Y_Gynoid_train = y_train["Gynoid"]
Y_Gynoid_train = Y_Gynoid_train.to_frame().rename(columns={'Gynoid': 'Gynoid'})

# åˆå¹¶Xå’ŒY
Gynoid_train_data = pd.concat([X_Gynoid_train, Y_Gynoid_train], axis=1)

# ä¸ºäº†å¯é‡å¤ï¼Œå¹¶ç»“åˆ VIF < 10 å’Œ å›å½’æ˜¾è‘—æ€§
## ä¿ç•™ [ "gender", "è†å›´åº¦å·¦è¾¹","é«˜è‡€éƒ¨å›´åº¦", "age", "å‰è‡‚å›´åº¦å³è¾¹", "å¤§è…¿å›´åº¦å³è¾¹.æ°´å¹³çº¿.", "è…¿è‚šå›´åº¦å³è¾¹", "å®½åº¦è…‹çª", "æ¨ªè¿‡åé¢å®½åº¦.è…‹çªæ°´å¹³.", "èƒ¯é•¿åº¦.åé¢", "ä¸Šè‡‚é•¿åº¦å·¦è¾¹"]

Gynoid_targets, Gynoid_forward_linear_model = feature_selection_process4(data = Gynoid_train_data, target_col= "Gynoid", significance_level=0.05, vif_threshold=10, r2_tolerance=0.01)

X_Gynoid_train_Update = X_train[Gynoid_targets]
vif_Gynoid_result_Update = calculate_vif(X_Gynoid_train_Update, include_const= False)  ## è®¡ç®—è‡ªå˜é‡çš„VIF
vif_Gynoid_result_Update.to_csv("C:/Users/zjl__/Desktop/output_zhaojialu/output/ForwardRegression2/VIF_Gynoid_params.csv", index=False)
print(vif_Gynoid_result_Update)

dw_Gynoid = durbin_watson(Gynoid_forward_linear_model.resid)
print(f"Gynoid-Durbin-Watsonç»Ÿè®¡é‡: {dw_Gynoid:.4f}") # Gynoid-Durbin-Watsonç»Ÿè®¡é‡: 2.0369


##  --------------------------------  Android / Gynoid  --------------------------

A_G_feature = AA_20240105_All_models['A_G_forward_linear_model']['Selected_feature']
X_A_G_train = X_train[A_G_feature]  ## è‡ªå˜é‡å€¼
Y_A_G_train = y_train["A_G"]
Y_A_G_train = Y_A_G_train.to_frame().rename(columns={'A_G': 'A_G'})

# åˆå¹¶Xå’ŒY
A_G_train_data = pd.concat([X_A_G_train, Y_A_G_train], axis=1)

# ä¸ºäº†å¯é‡å¤ï¼Œå¹¶ç»“åˆ VIF < 10 å’Œ å›å½’æ˜¾è‘—æ€§
## åˆ é™¤['é«˜è…°å›´','æ ‡å‡†ç«™å§¿è…°å›´','Underbuståœ†å‘¨.æ°´å¹³çº¿.','è…°å¸¦.è†è·ç¦»','è…°å¸¦.è…°çš„åç§»é‡.å‰é¢.','7CV.å‚ç›´çš„è·ç¦»','è„–å­å‰é¢åˆ°å‚ç›´çš„è·ç¦»','èƒ¸éƒ¨.èƒ¸å›´åº¦.æ°´å¹³çº¿.','è„–å­åˆ°æ¨ªè¿‡åé¢å®½åº¦.è…‹çªæ°´å¹³.','ä¸Šé¢çš„èº¯å¹²æ‰­...']
## ä¿ç•™ ["è‡€éƒ¨å›´åº¦", "å¯¹é«˜è‡€éƒ¨çš„è…°éƒ¨åé¢", "å¤§è…¿å›´åº¦å·¦è¾¹.æ°´å¹³çº¿.", "è„šè¸å›´å·¦è¾¹", "ä¸­é—´çš„è„–å­å›´åº¦", "æ ‡å‡†ç«™å§¿è…°å›´", "èƒ¸éƒ¨.èƒ¸å›´åº¦", "è„–å­åˆ°æ¨ªè¿‡åé¢å®½åº¦.è…‹çªæ°´å¹³", "å‰è‡‚é•¿åº¦å·¦è¾¹"]

A_G_targets, A_G_forward_linear_model = feature_selection_process4(data = A_G_train_data, target_col= "A_G", significance_level=0.05, vif_threshold=10, r2_tolerance=0.01)

X_A_G_train_Update = X_train[A_G_targets]
vif_A_G_result_Update = calculate_vif(X_A_G_train_Update, include_const= False)  ## è®¡ç®—è‡ªå˜é‡çš„VIF
vif_A_G_result_Update.to_csv("C:/Users/zjl__/Desktop/output_zhaojialu/output/ForwardRegression2/VIF_A_G__params.csv", index=False)
print(vif_A_G_result_Update)

dw_A_G = durbin_watson(A_G_forward_linear_model.resid)
print(f"A_G-Durbin-Watsonç»Ÿè®¡é‡: {dw_A_G:.4f}") # A_G-Durbin-Watsonç»Ÿè®¡é‡: 1.8479

##  --------------------------------  BFP  --------------------------

BFP_feature = AA_20240105_All_models['BFP_forward_linear_model']['Selected_feature']
X_BFP_train = X_train[BFP_feature]  ## è‡ªå˜é‡å€¼
Y_BFP_train = y_train["BFP"] * 100
Y_BFP_train = Y_BFP_train.to_frame().rename(columns={'BFP': 'BFP'})

# åˆå¹¶Xå’ŒY
BFP_train_data = pd.concat([X_BFP_train, Y_BFP_train], axis=1)

# ä¸ºäº†å¯é‡å¤ï¼Œå¹¶ç»“åˆ VIF < 10 å’Œ å›å½’æ˜¾è‘—æ€§
# åˆ é™¤ ['ä¸­è‡€å›´','è…¹åœ†å‘¨','è„–å­å¯¹è…°éƒ¨ä¸­å¿ƒå‘ååœ°','7CV.å‚ç›´çš„è·ç¦»','è„–å­å‰é¢åˆ°å‚ç›´çš„è·ç¦»','age','ä¸­é—´çš„è„–å­å›´åº¦','è…°å¸¦.è…°çš„åç§»é‡.å‰é¢.','åèƒŒåˆ°å‚ç›´é¢è·ç¦».åœ¨èƒ¸éƒ¨æ°´å¹³é¢','é«˜è…°å›´','è„–å­æ­£ç¡®åœ°å¯¹è…°éƒ¨åé¢','å¯¹é«˜è‡€éƒ¨çš„è…°éƒ¨åé¢','è‡€éƒ¨å›´åº¦','weight','èƒ¸éƒ¨åˆ°å‚ç›´çš„è·ç¦»','è‚©è§’åº¦å³è¾¹','æ¨ªè‚©è¦†ç›–è„–å­']
# ä¿ç•™["gender", "è‡‚é•¿å·¦", "å‰è‡‚å›´åº¦å³è¾¹", "è†å›´åº¦å·¦è¾¹", "è„šè¸å›´å·¦è¾¹", "è‡€éƒ¨.å¤§è…¿å›´åº¦",  "å¤´é«˜",  "è…¿è‚šå›´åº¦å·¦è¾¹",  "æœ€å°è…¿å›´åº¦å·¦è¾¹", "å¤§è…¿å›´åº¦å³è¾¹.æ°´å¹³çº¿.","å®½åº¦è…‹çª", "è„–å­å·¦è¾¹å¯¹è…°éƒ¨åé¢", "æ¨ªè¿‡åé¢å®½åº¦.è…‹çªæ°´å¹³.", "æ ‡å‡†ç«™å§¿è…°å›´",  "é¢ˆåˆ°è‡€è·ç¦»","èƒ¯é•¿åº¦.åé¢"]

BFP_targets, BFP_forward_linear_model = feature_selection_process4(data = BFP_train_data, target_col= "BFP", significance_level=0.05, vif_threshold=10, r2_tolerance=0.01)

X_BFP_train_Update = X_train[BFP_targets]
vif_BFP_result_Update = calculate_vif(X_BFP_train_Update, include_const= False)  ## è®¡ç®—è‡ªå˜é‡çš„VIF
vif_BFP_result_Update.to_csv("C:/Users/zjl__/Desktop/output_zhaojialu/output/ForwardRegression2/VIF_BFP__params.csv", index=False)
print(vif_BFP_result_Update)

dw_BFP = durbin_watson(BFP_forward_linear_model.resid)
print(f"BFP-Durbin-Watsonç»Ÿè®¡é‡: {dw_BFP:.4f}") # BFP-Durbin-Watsonç»Ÿè®¡é‡: 1.9218

#########  è·å¾—æ¨¡å‹å‚æ•°å’Œæµ‹è¯•é›†æ¨¡å‹æ€§èƒ½ ################

def model_result(X_final, Y_final, final_model):
    params = final_model.params  ## è·å–æ¨¡å‹å‚æ•°
    p_values = final_model.pvalues  ## è·å–På€¼
    r_squared = final_model.rsquared  ## è·å–R2
    y_pred = final_model.predict(X_final)  ## è·å–Yçš„é¢„æµ‹å€¼
    rmse = np.sqrt(mean_squared_error(Y_final, y_pred))  ## è®¡ç®—RMSE
    print("r2:",r_squared, "rmse:",rmse)

    results = pd.DataFrame({
        'Parameter': params.index,
        'Coefficient': params.values,
        'P-value': p_values.values
    })

    r2_row = pd.DataFrame({
        'Parameter': ['R-Squared'],
        'Coefficient': [r_squared],
        'P-Value': [None]
    })  ## æ·»åŠ R2

    rmse_row = pd.DataFrame({
        'Parameter': ['RMSE'],
        'Coefficient': [rmse],
        'P-Value': [None]
    })  ## æ·»åŠ RMSE

    results = pd.concat([results, r2_row, rmse_row], ignore_index=True)

    results['Coefficient'] = results['Coefficient'].round(3)
    
    return results

X_VAT_test = X_test[VAT_targets]
X_VAT_test = add_constant(X_VAT_test)
Y_VAT_test = y_test['VATmass']
print("VATmass") # r2: 0.8058659068946479 rmse: 0.23225482234553405
VIF_VAT_params = model_result(X_VAT_test, Y_VAT_test, VAT_forward_linear_model)
VIF_VAT_params.to_csv("C:/Users/zjl__/Desktop/output_zhaojialu/output/Coefficient2/VIF_VAT_params.csv", index=False)

X_FM_test = X_test[FM_targets]
X_FM_test = add_constant(X_FM_test)
Y_FM_test = y_test['FM']
print("FM")  # r2: 0.8766785992784824 rmse: 2.1806469681696576
VIF_FM_params = model_result(X_FM_test, Y_FM_test, FM_forward_linear_model)
VIF_FM_params.to_csv("C:/Users/zjl__/Desktop/output_zhaojialu/output/Coefficient2/VIF_FM_params.csv", index=False)

X_FMI_test = X_test[FMI_targets]
X_FMI_test = add_constant(X_FMI_test)
Y_FMI_test = y_test['FMI']
print("FMI") # r2: 0.8676449370555529 rmse: 0.798729658671095
VIF_FMI_params = model_result(X_FMI_test, Y_FMI_test, FMI_forward_linear_model)
VIF_FMI_params.to_csv("C:/Users/zjl__/Desktop/output_zhaojialu/output/Coefficient2/VIF_FMI_params.csv", index=False)

X_LM_test = X_test[LM_targets]
X_LM_test = add_constant(X_LM_test)
Y_LM_test = y_test['LM']
print("LM")  # 0.9210978254059097 rmse: 2.488947923219758
VIF_LM_params = model_result(X_LM_test, Y_LM_test, LM_forward_linear_model)
VIF_LM_params.to_csv("C:/Users/zjl__/Desktop/output_zhaojialu/output/Coefficient2/VIF_LM_params.csv", index=False)

X_Android_test = X_test[Android_targets]
X_Android_test = add_constant(X_Android_test)
Y_Android_test = y_test['Android']
print("Android")  # r2: 0.8852250182118173 rmse: 0.2596830447532017
VIF_Android_params = model_result(X_Android_test, Y_Android_test, Android_forward_linear_model)
VIF_Android_params.to_csv("C:/Users/zjl__/Desktop/output_zhaojialu/output/Coefficient2/VIF_Android_params.csv", index=False)

X_Gynoid_test = X_test[Gynoid_targets]
X_Gynoid_test = add_constant(X_Gynoid_test)
Y_Gynoid_test = y_test['Gynoid']
print("Gynoid")  # 0.8660905559562025 rmse: 0.37364140066743995
VIF_Gynoid_params = model_result(X_Gynoid_test, Y_Gynoid_test, Gynoid_forward_linear_model)
VIF_Gynoid_params.to_csv("C:/Users/zjl__/Desktop/output_zhaojialu/output/Coefficient2/VIF_Gynoid_params.csv", index=False)

X_A_G_test = X_test[A_G_targets]
X_A_G_test = add_constant(X_A_G_test)
Y_A_G_test = y_test['A_G']
print("AGFMR")  # r2: 0.8438780901807932 rmse: 0.07848113372038507
VIF_A_G_params = model_result(X_A_G_test, Y_A_G_test, A_G_forward_linear_model)
VIF_A_G_params.to_csv("C:/Users/zjl__/Desktop/output_zhaojialu/output/Coefficient2/VIF_A_G_params.csv", index=False)

X_BFP_test = X_test[BFP_targets]
X_BFP_test = add_constant(X_BFP_test)
Y_BFP_test = y_test['BFP'] * 100
print("BFP")   # r2: 0.7872491935850359 rmse: 3.374100525171791
VIF_BFP_params = model_result(X_BFP_test, Y_BFP_test, BFP_forward_linear_model)
VIF_BFP_params.to_csv("C:/Users/zjl__/Desktop/output_zhaojialu/output/Coefficient2/VIF_BFP_params.csv", index=False)


## å°†å¤šä¸ªæ¨¡å‹å’Œå‚æ•°ä¿å­˜åˆ°ä¸€ä¸ªå­—å…¸ä¸­
AA_20240105_All_VIF_models = {
    'FMI_forward_linear_model':{ 'Forward_linear_model': FMI_forward_linear_model, 'Selected_feature': FMI_targets},
    'A_G_forward_linear_model':{ 'Forward_linear_model': A_G_forward_linear_model, 'Selected_feature': A_G_targets},
    'FM_forward_linear_model':{ 'Forward_linear_model' : FM_forward_linear_model, 'Selected_feature': FM_targets},
    'LM_forward_linear_model':{ 'Forward_linear_model' : LM_forward_linear_model, 'Selected_feature': LM_targets},
    'VAT_forward_linear_model':{ 'Forward_linear_model' : VAT_forward_linear_model, 'Selected_feature': VAT_targets},
    'Android_forward_linear_model':{ 'Forward_linear_model' : Android_forward_linear_model, 'Selected_feature': Android_targets},
    'Gynoid_forward_linear_model':{ 'Forward_linear_model' : Gynoid_forward_linear_model, 'Selected_feature': Gynoid_targets},
    'BFP_forward_linear_model': { 'Forward_linear_model' : BFP_forward_linear_model, 'Selected_feature': BFP_targets}
}

## ä¿å­˜å­—å…¸

dump(AA_20240105_All_VIF_models, 'C:/Users/zjl__/Desktop/output_zhaojialu/output/ForwardRegression2/AA_20240105_All_VIF_models.joblib')


####################### æ¯”è¾ƒæ¨¡å‹åœ¨è®­ç»ƒé›†å’Œæµ‹è¯•é›†çš„æ€§èƒ½æŒ‡æ ‡ R2ï¼ŒRMSEï¼ŒME(95%CI) #########################33

## åŠ è½½ä¹‹å‰ç”¨è®­ç»ƒé›†è®­ç»ƒçš„å¤šä¸ªæ¨¡å‹åŠå‚æ•°
AA_20240105_All_VIF_models = load('C:/Users/zjl__/Desktop/output_zhaojialu/output/ForwardRegression2/AA_20240105_All_VIF_models.joblib')


##å®šä¹‰è®¡ç®—æ¨¡å‹æ€§èƒ½çš„å‡½æ•°
def calculate_metrics(y_true, y_pred):
    """
    è®¡ç®—å¹¶è¿”å› RÂ², RMSE, ME å’Œ ME çš„ 95% CI
    """
    n = len(y_true)
    
    # R-squared
    r2 = r2_score(y_true, y_pred)
    
    # RMSE
    rmse = np.sqrt(mean_squared_error(y_true, y_pred))
    
    # Mean Error (ME)
    errors =  y_pred - y_true
    me = np.mean(errors)
    
    # 95% Confidence Interval for ME
    se_me = stats.sem(errors) # stats.sem ç›´æ¥è®¡ç®—æ ‡å‡†è¯¯
    ci_low, ci_high = stats.t.interval(0.95, df=n-1, loc=me, scale=se_me)
    
    return r2, rmse, me, (ci_low, ci_high)

################################   FMI  ######################################
FMI_FSLR_VIF_model = AA_20240105_All_VIF_models['FMI_forward_linear_model']['Forward_linear_model']
FMI_Selected_feature = AA_20240105_All_VIF_models['FMI_forward_linear_model']['Selected_feature']
FMI_train_pred = FMI_FSLR_VIF_model.predict(add_constant(X_train[FMI_Selected_feature]))
FMI_test_pred = FMI_forward_linear_model.predict(add_constant(X_test[FMI_Selected_feature]))


#  å¯¹è®­ç»ƒé›†å’Œæµ‹è¯•é›†åˆ†åˆ«è®¡ç®—æŒ‡æ ‡
FMI_train_r2, FMI_train_rmse, FMI_train_me, FMI_train_ci = calculate_metrics(y_train['FMI'], FMI_train_pred)
FMI_test_r2, FMI_test_rmse, FMI_test_me, FMI_test_ci = calculate_metrics(y_test['FMI'], FMI_test_pred)

################################   A_G  ######################################
A_G_FSLR_VIF_model = AA_20240105_All_VIF_models['A_G_forward_linear_model']['Forward_linear_model']
A_G_Selected_feature = AA_20240105_All_VIF_models['A_G_forward_linear_model']['Selected_feature']
A_G_train_pred = A_G_FSLR_VIF_model.predict(add_constant(X_train[A_G_Selected_feature]))
A_G_test_pred = A_G_forward_linear_model.predict(add_constant(X_test[A_G_Selected_feature]))


#  å¯¹è®­ç»ƒé›†å’Œæµ‹è¯•é›†åˆ†åˆ«è®¡ç®—æŒ‡æ ‡
A_G_train_r2, A_G_train_rmse, A_G_train_me, A_G_train_ci = calculate_metrics(y_train['A_G'], A_G_train_pred)
A_G_test_r2, A_G_test_rmse, A_G_test_me, A_G_test_ci = calculate_metrics(y_test['A_G'], A_G_test_pred)

################################   FM  ######################################
FM_FSLR_VIF_model = AA_20240105_All_VIF_models['FM_forward_linear_model']['Forward_linear_model']
FM_Selected_feature = AA_20240105_All_VIF_models['FM_forward_linear_model']['Selected_feature']
FM_train_pred = FM_FSLR_VIF_model.predict(add_constant(X_train[FM_Selected_feature]))
FM_test_pred = FM_forward_linear_model.predict(add_constant(X_test[FM_Selected_feature]))

#  å¯¹è®­ç»ƒé›†å’Œæµ‹è¯•é›†åˆ†åˆ«è®¡ç®—æŒ‡æ ‡
FM_train_r2, FM_train_rmse, FM_train_me, FM_train_ci = calculate_metrics(y_train['FM'], FM_train_pred)
FM_test_r2, FM_test_rmse, FM_test_me, FM_test_ci = calculate_metrics(y_test['FM'], FM_test_pred)

################################   LM  ######################################
LM_FSLR_VIF_model = AA_20240105_All_VIF_models['LM_forward_linear_model']['Forward_linear_model']
LM_Selected_feature = AA_20240105_All_VIF_models['LM_forward_linear_model']['Selected_feature']
LM_train_pred = LM_FSLR_VIF_model.predict(add_constant(X_train[LM_Selected_feature]))
LM_test_pred = LM_forward_linear_model.predict(add_constant(X_test[LM_Selected_feature]))

#  å¯¹è®­ç»ƒé›†å’Œæµ‹è¯•é›†åˆ†åˆ«è®¡ç®—æŒ‡æ ‡
LM_train_r2, LM_train_rmse, LM_train_me, LM_train_ci = calculate_metrics(y_train['LM'], LM_train_pred)
LM_test_r2, LM_test_rmse, LM_test_me, LM_test_ci = calculate_metrics(y_test['LM'], LM_test_pred)

################################  VAT  ######################################
VAT_FSLR_VIF_model = AA_20240105_All_VIF_models['VAT_forward_linear_model']['Forward_linear_model']
VAT_Selected_feature = AA_20240105_All_VIF_models['VAT_forward_linear_model']['Selected_feature']
VAT_train_pred = VAT_FSLR_VIF_model.predict(add_constant(X_train[VAT_Selected_feature]))
VAT_test_pred = VAT_forward_linear_model.predict(add_constant(X_test[VAT_Selected_feature]))

#  å¯¹è®­ç»ƒé›†å’Œæµ‹è¯•é›†åˆ†åˆ«è®¡ç®—æŒ‡æ ‡
VAT_train_r2, VAT_train_rmse, VAT_train_me, VAT_train_ci = calculate_metrics(y_train['VATmass'], VAT_train_pred)
VAT_test_r2, VAT_test_rmse, VAT_test_me, VAT_test_ci = calculate_metrics(y_test['VATmass'], VAT_test_pred)

################################  Android  ######################################
Android_FSLR_VIF_model = AA_20240105_All_VIF_models['Android_forward_linear_model']['Forward_linear_model']
Android_Selected_feature = AA_20240105_All_VIF_models['Android_forward_linear_model']['Selected_feature']
Android_train_pred = Android_FSLR_VIF_model.predict(add_constant(X_train[Android_Selected_feature]))
Android_test_pred = Android_forward_linear_model.predict(add_constant(X_test[Android_Selected_feature]))

#  å¯¹è®­ç»ƒé›†å’Œæµ‹è¯•é›†åˆ†åˆ«è®¡ç®—æŒ‡æ ‡
Android_train_r2, Android_train_rmse, Android_train_me, Android_train_ci = calculate_metrics(y_train['Android'], Android_train_pred)
Android_test_r2, Android_test_rmse, Android_test_me, Android_test_ci = calculate_metrics(y_test['Android'], Android_test_pred)


################################  Gynoid  ######################################
Gynoid_FSLR_VIF_model = AA_20240105_All_VIF_models['Gynoid_forward_linear_model']['Forward_linear_model']
Gynoid_Selected_feature = AA_20240105_All_VIF_models['Gynoid_forward_linear_model']['Selected_feature']
Gynoid_train_pred = Gynoid_FSLR_VIF_model.predict(add_constant(X_train[Gynoid_Selected_feature]))
Gynoid_test_pred = Gynoid_forward_linear_model.predict(add_constant(X_test[Gynoid_Selected_feature]))

#  å¯¹è®­ç»ƒé›†å’Œæµ‹è¯•é›†åˆ†åˆ«è®¡ç®—æŒ‡æ ‡
Gynoid_train_r2, Gynoid_train_rmse, Gynoid_train_me, Gynoid_train_ci = calculate_metrics(y_train['Gynoid'], Gynoid_train_pred)
Gynoid_test_r2, Gynoid_test_rmse, Gynoid_test_me, Gynoid_test_ci = calculate_metrics(y_test['Gynoid'], Gynoid_test_pred)

################################  BFP ######################################
BFP_FSLR_VIF_model = AA_20240105_All_VIF_models['BFP_forward_linear_model']['Forward_linear_model']
BFP_Selected_feature = AA_20240105_All_VIF_models['BFP_forward_linear_model']['Selected_feature']
BFP_train_pred = BFP_FSLR_VIF_model.predict(add_constant(X_train[BFP_Selected_feature]))
BFP_test_pred = BFP_forward_linear_model.predict(add_constant(X_test[BFP_Selected_feature]))

#  å¯¹è®­ç»ƒé›†å’Œæµ‹è¯•é›†åˆ†åˆ«è®¡ç®—æŒ‡æ ‡
BFP_train_r2, BFP_train_rmse, BFP_train_me, BFP_train_ci = calculate_metrics(y_train['BFP'] *100, BFP_train_pred)
BFP_test_r2, BFP_test_rmse, BFP_test_me, BFP_test_ci = calculate_metrics(y_test['BFP'] * 100, BFP_test_pred)


df = {
    'Outcome': ['Android', 'Gyroid', 'AGFMR', 'FM', 'FMI', 'BFP', 'LM', 'VAT'],
    'Training_R2': [Android_train_r2, Gynoid_train_r2, A_G_train_r2, FM_train_r2, 
                    FMI_train_r2, BFP_train_r2, LM_train_r2, VAT_train_r2],
    'Training_RMSE': [Android_train_rmse, Gynoid_train_rmse, A_G_train_rmse, FM_train_rmse, 
                      FMI_train_rmse, BFP_train_rmse, LM_train_rmse, VAT_train_rmse],
    'Training_ME': [Android_train_me, Gynoid_train_me, A_G_train_me, FM_train_me,
                    FMI_train_me, BFP_train_me, LM_train_me, VAT_train_me],
    'Training_ME_CI': [Android_train_ci, Gynoid_train_ci, A_G_train_ci, FM_train_ci, 
                       FMI_train_ci, BFP_train_ci, LM_train_ci, VAT_train_ci],
    'Test_R2': [Android_test_r2, Gynoid_test_r2, A_G_test_r2, FM_test_r2, 
                FMI_test_r2, BFP_test_r2, LM_test_r2, VAT_test_r2],
    'Test_RMSE': [Android_test_rmse, Gynoid_test_rmse, A_G_test_rmse, FM_test_rmse, 
                  FMI_test_rmse, BFP_test_rmse, LM_test_rmse, VAT_test_rmse],
    'Test_ME': [Android_test_me, Gynoid_test_me, A_G_test_me, FM_test_me,
                FMI_test_me, BFP_test_me, LM_test_me, VAT_test_me],
    'Test_ME_CI': [Android_test_ci, Gynoid_test_ci, A_G_test_ci, 
                  FM_test_ci, FMI_test_ci, BFP_test_ci, LM_test_ci, VAT_test_ci]
}

# å°†å­—å…¸è½¬æ¢ä¸ºDataFrame
df = pd.DataFrame(df)
df.to_csv("C:/Users/zjl__/Desktop/output_zhaojialu/output/ForwardRegression2/Model_evaluate_indices.csv", index=False)


